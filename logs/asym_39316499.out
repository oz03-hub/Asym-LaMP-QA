Keeping [] in profile
Temp data file saved at data/processed/TEMP_ae_processed_test.json.
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_ae_processed_test.json', '--output_addr', 'data/out/nopers/ae_no_pers_test_output.json', '--temperature', '0.0', '--top_p', '0.95', '--max_tokens', '4096', '--num_generated_outputs', '1', '--num_contexts', '5', '--max_retries', '5', '--cache_dir', '/work/pi_hzamani_umass_edu/ozel_cache/']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 401 examples [00:00, 9738.21 examples/s]
INFO 07-03 18:51:10 config.py:510] This model supports multiple tasks: {'reward', 'score', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 07-03 18:51:10 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/work/pi_hzamani_umass_edu/ozel_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-03 18:51:14 selector.py:120] Using Flash Attention backend.
INFO 07-03 18:51:15 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-03 18:51:15 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.49s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:16<00:16,  8.45s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:24<00:08,  8.11s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:31<00:00,  7.80s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:31<00:00,  7.99s/it]

INFO 07-03 18:52:48 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-03 18:52:51 worker.py:241] Memory profiling takes 2.55 seconds
INFO 07-03 18:52:51 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-03 18:52:51 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-03 18:52:51 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-03 18:52:51 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-03 18:52:54 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:14,  2.29it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:12,  2.67it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:11,  2.82it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:10,  2.89it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:10,  2.94it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:09,  2.97it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:09,  2.99it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:08,  3.01it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:08,  2.92it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:03<00:08,  2.96it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:03<00:08,  2.99it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:04<00:07,  3.01it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:04<00:07,  3.02it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:04<00:06,  3.04it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:05<00:06,  3.05it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:05<00:06,  3.05it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:05<00:05,  3.08it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:06<00:05,  3.10it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:06<00:05,  3.11it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:06<00:04,  3.12it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:06<00:04,  3.12it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:07<00:04,  3.13it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:07<00:03,  3.14it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:07<00:03,  3.14it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:08<00:03,  3.14it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:08<00:02,  3.15it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:08<00:02,  3.15it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:09<00:02,  3.15it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:09<00:01,  3.16it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:09<00:01,  3.17it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:10<00:01,  3.13it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:10<00:00,  3.15it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:10<00:00,  3.06it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:11<00:00,  3.10it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.12it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.05it/s]
INFO 07-03 18:53:05 model_runner.py:1535] Graph capturing finished in 11 secs, took 0.22 GiB
INFO 07-03 18:53:05 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 17.10 seconds
None

Processed prompts:   0%|          | 0/401 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/401 [00:02<14:53,  2.23s/it, est. speed input: 34.04 toks/s, output: 12.99 toks/s]
Processed prompts:   0%|          | 2/401 [00:02<07:34,  1.14s/it, est. speed input: 55.24 toks/s, output: 26.09 toks/s]
Processed prompts:   1%|          | 4/401 [00:02<03:10,  2.08it/s, est. speed input: 107.16 toks/s, output: 54.30 toks/s]
Processed prompts:   2%|▏         | 7/401 [00:02<01:31,  4.32it/s, est. speed input: 174.19 toks/s, output: 97.19 toks/s]
Processed prompts:   3%|▎         | 11/401 [00:03<01:00,  6.48it/s, est. speed input: 244.18 toks/s, output: 144.97 toks/s]
Processed prompts:   3%|▎         | 14/401 [00:03<00:46,  8.32it/s, est. speed input: 298.51 toks/s, output: 180.85 toks/s]
Processed prompts:   4%|▍         | 17/401 [00:03<00:36, 10.46it/s, est. speed input: 348.40 toks/s, output: 218.66 toks/s]
Processed prompts:   5%|▌         | 22/401 [00:03<00:23, 15.81it/s, est. speed input: 435.25 toks/s, output: 287.91 toks/s]
Processed prompts:   7%|▋         | 30/401 [00:03<00:14, 25.38it/s, est. speed input: 571.15 toks/s, output: 399.17 toks/s]
Processed prompts:   9%|▉         | 38/401 [00:03<00:10, 33.92it/s, est. speed input: 705.98 toks/s, output: 508.07 toks/s]
Processed prompts:  11%|█         | 43/401 [00:04<00:09, 36.00it/s, est. speed input: 776.88 toks/s, output: 570.10 toks/s]
Processed prompts:  13%|█▎        | 54/401 [00:04<00:07, 47.79it/s, est. speed input: 948.04 toks/s, output: 717.33 toks/s]
Processed prompts:  16%|█▌        | 65/401 [00:04<00:05, 56.85it/s, est. speed input: 1108.85 toks/s, output: 860.83 toks/s]
Processed prompts:  18%|█▊        | 72/401 [00:04<00:05, 57.14it/s, est. speed input: 1197.16 toks/s, output: 943.74 toks/s]
Processed prompts:  20%|█▉        | 79/401 [00:04<00:05, 58.91it/s, est. speed input: 1281.66 toks/s, output: 1029.26 toks/s]
Processed prompts:  22%|██▏       | 87/401 [00:04<00:05, 58.67it/s, est. speed input: 1369.28 toks/s, output: 1121.28 toks/s]
Processed prompts:  23%|██▎       | 94/401 [00:04<00:05, 58.47it/s, est. speed input: 1441.85 toks/s, output: 1200.05 toks/s]
Processed prompts:  26%|██▌       | 103/401 [00:04<00:04, 61.77it/s, est. speed input: 1535.82 toks/s, output: 1306.19 toks/s]
Processed prompts:  28%|██▊       | 114/401 [00:05<00:04, 68.14it/s, est. speed input: 1655.04 toks/s, output: 1439.27 toks/s]
Processed prompts:  30%|███       | 121/401 [00:05<00:04, 67.02it/s, est. speed input: 1721.28 toks/s, output: 1517.17 toks/s]
Processed prompts:  32%|███▏      | 128/401 [00:05<00:04, 62.07it/s, est. speed input: 1774.15 toks/s, output: 1586.09 toks/s]
Processed prompts:  34%|███▍      | 137/401 [00:05<00:04, 64.47it/s, est. speed input: 1857.18 toks/s, output: 1687.26 toks/s]
Processed prompts:  36%|███▋      | 146/401 [00:05<00:03, 65.90it/s, est. speed input: 1932.28 toks/s, output: 1785.96 toks/s]
Processed prompts:  40%|███▉      | 160/401 [00:05<00:02, 82.86it/s, est. speed input: 2080.77 toks/s, output: 1968.93 toks/s]
Processed prompts:  42%|████▏     | 169/401 [00:05<00:02, 83.26it/s, est. speed input: 2159.04 toks/s, output: 2074.89 toks/s]
Processed prompts:  44%|████▍     | 178/401 [00:05<00:02, 77.13it/s, est. speed input: 2219.70 toks/s, output: 2163.90 toks/s]
Processed prompts:  48%|████▊     | 192/401 [00:06<00:02, 93.27it/s, est. speed input: 2360.34 toks/s, output: 2331.57 toks/s]
Processed prompts:  52%|█████▏    | 208/401 [00:06<00:01, 103.39it/s, est. speed input: 2508.21 toks/s, output: 2537.34 toks/s]
Processed prompts:  55%|█████▌    | 222/401 [00:06<00:01, 107.00it/s, est. speed input: 2634.65 toks/s, output: 2708.29 toks/s]
Processed prompts:  58%|█████▊    | 233/401 [00:06<00:01, 103.23it/s, est. speed input: 2718.51 toks/s, output: 2832.12 toks/s]
Processed prompts:  61%|██████    | 244/401 [00:06<00:01, 78.79it/s, est. speed input: 2752.91 toks/s, output: 2901.04 toks/s] 
Processed prompts:  64%|██████▍   | 256/401 [00:06<00:01, 85.89it/s, est. speed input: 2845.13 toks/s, output: 3002.52 toks/s]
Processed prompts:  66%|██████▋   | 266/401 [00:06<00:01, 87.72it/s, est. speed input: 2912.75 toks/s, output: 3073.52 toks/s]
Processed prompts:  69%|██████▉   | 276/401 [00:06<00:01, 85.45it/s, est. speed input: 2967.69 toks/s, output: 3112.56 toks/s]
Processed prompts:  71%|███████   | 285/401 [00:07<00:01, 79.57it/s, est. speed input: 3004.86 toks/s, output: 3169.45 toks/s]
Processed prompts:  73%|███████▎  | 294/401 [00:07<00:01, 80.29it/s, est. speed input: 3054.01 toks/s, output: 3227.72 toks/s]
Processed prompts:  76%|███████▌  | 304/401 [00:07<00:01, 84.23it/s, est. speed input: 3110.84 toks/s, output: 3304.82 toks/s]
Processed prompts:  78%|███████▊  | 313/401 [00:07<00:01, 85.37it/s, est. speed input: 3157.97 toks/s, output: 3370.68 toks/s]
Processed prompts:  82%|████████▏ | 330/401 [00:07<00:00, 103.88it/s, est. speed input: 3277.79 toks/s, output: 3527.99 toks/s]
Processed prompts:  86%|████████▌ | 343/401 [00:07<00:00, 108.70it/s, est. speed input: 3362.47 toks/s, output: 3654.71 toks/s]
Processed prompts:  88%|████████▊ | 354/401 [00:07<00:00, 99.49it/s, est. speed input: 3413.08 toks/s, output: 3734.64 toks/s] 
Processed prompts:  91%|█████████▏| 366/401 [00:07<00:00, 101.67it/s, est. speed input: 3480.55 toks/s, output: 3839.66 toks/s]
Processed prompts:  94%|█████████▍| 377/401 [00:08<00:00, 92.15it/s, est. speed input: 3525.02 toks/s, output: 3949.18 toks/s] 
Processed prompts:  97%|█████████▋| 387/401 [00:08<00:00, 77.20it/s, est. speed input: 3544.37 toks/s, output: 4011.46 toks/s]
Processed prompts:  99%|█████████▉| 396/401 [00:08<00:00, 60.83it/s, est. speed input: 3530.16 toks/s, output: 4052.85 toks/s]
Processed prompts: 100%|█████████▉| 400/401 [00:19<00:00, 60.83it/s, est. speed input: 3076.81 toks/s, output: 3580.59 toks/s]
Processed prompts: 100%|██████████| 401/401 [00:53<00:00,  1.55s/it, est. speed input: 564.92 toks/s, output: 732.16 toks/s]  
Processed prompts: 100%|██████████| 401/401 [00:53<00:00,  7.47it/s, est. speed input: 564.92 toks/s, output: 732.16 toks/s]
{  "personalized_answer": "Christoff's alternate plan, as seen in the game 'BioShock 2', involved using the Adam he had saved from the璇汯槑弬槑槑桭槑桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭桭
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 64.51 toks/s, output: 77.23 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 64.51 toks/s, output: 77.23 toks/s]
[rank0]:[W703 18:54:02.777387957 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/nopers/ae_no_pers_test_output.json
Cleaned temporary files.
Keeping [] in profile
Temp data file saved at data/processed/TEMP_lp_processed_test.json.
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_lp_processed_test.json', '--output_addr', 'data/out/nopers/lp_test_output.json', '--temperature', '0.0', '--top_p', '0.95', '--max_tokens', '4096', '--num_generated_outputs', '1', '--num_contexts', '5', '--max_retries', '5', '--cache_dir', '/work/pi_hzamani_umass_edu/ozel_cache/']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 526 examples [00:00, 10823.59 examples/s]
INFO 07-03 18:54:26 config.py:510] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.
INFO 07-03 18:54:26 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/work/pi_hzamani_umass_edu/ozel_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-03 18:54:27 selector.py:120] Using Flash Attention backend.
INFO 07-03 18:54:28 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-03 18:54:28 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.76it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.66it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.60it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.64it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.64it/s]

INFO 07-03 18:54:31 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-03 18:54:33 worker.py:241] Memory profiling takes 2.09 seconds
INFO 07-03 18:54:33 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-03 18:54:33 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-03 18:54:33 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-03 18:54:33 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-03 18:54:36 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:12,  2.67it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:11,  2.85it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:11,  2.91it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:10,  2.94it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:10,  2.96it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:09,  2.98it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:09,  2.98it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:09,  2.99it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:08,  2.98it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:03<00:08,  2.99it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:03<00:07,  3.00it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:04<00:07,  3.01it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:04<00:07,  3.01it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:04<00:06,  3.02it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:05<00:06,  3.02it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:05<00:06,  3.02it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:05<00:05,  3.05it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:06<00:05,  3.07it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:06<00:05,  3.08it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:06<00:04,  3.04it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:06<00:04,  3.05it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:07<00:04,  3.07it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:07<00:03,  3.08it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:07<00:03,  3.10it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:08<00:03,  3.11it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:08<00:02,  3.11it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:08<00:02,  3.07it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:09<00:02,  3.07it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:09<00:01,  3.09it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:09<00:01,  3.11it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:10<00:01,  3.12it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:10<00:00,  3.13it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:10<00:00,  3.08it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:11<00:00,  3.11it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.12it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.05it/s]
INFO 07-03 18:54:48 model_runner.py:1535] Graph capturing finished in 11 secs, took 0.22 GiB
INFO 07-03 18:54:48 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 16.65 seconds
None

Processed prompts:   0%|          | 0/526 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/526 [00:03<26:40,  3.05s/it, est. speed input: 23.29 toks/s, output: 15.75 toks/s]
Processed prompts:   1%|          | 3/526 [00:03<07:13,  1.21it/s, est. speed input: 67.86 toks/s, output: 46.61 toks/s]
Processed prompts:   1%|          | 6/526 [00:03<02:57,  2.92it/s, est. speed input: 132.83 toks/s, output: 92.03 toks/s]
Processed prompts:   2%|▏         | 8/526 [00:03<02:12,  3.92it/s, est. speed input: 166.12 toks/s, output: 118.62 toks/s]
Processed prompts:   2%|▏         | 10/526 [00:03<01:35,  5.40it/s, est. speed input: 200.97 toks/s, output: 147.86 toks/s]
Processed prompts:   3%|▎         | 15/526 [00:03<00:48, 10.49it/s, est. speed input: 293.55 toks/s, output: 224.70 toks/s]
Processed prompts:   3%|▎         | 18/526 [00:03<00:38, 13.12it/s, est. speed input: 345.03 toks/s, output: 267.60 toks/s]
Processed prompts:   4%|▍         | 22/526 [00:03<00:29, 17.31it/s, est. speed input: 411.53 toks/s, output: 325.53 toks/s]
Processed prompts:   5%|▍         | 25/526 [00:04<00:25, 19.57it/s, est. speed input: 454.45 toks/s, output: 366.95 toks/s]
Processed prompts:   6%|▌         | 31/526 [00:04<00:18, 26.92it/s, est. speed input: 545.86 toks/s, output: 454.88 toks/s]
Processed prompts:   7%|▋         | 35/526 [00:04<00:16, 29.20it/s, est. speed input: 598.40 toks/s, output: 509.07 toks/s]
Processed prompts:   9%|▊         | 45/526 [00:04<00:11, 42.72it/s, est. speed input: 754.29 toks/s, output: 656.95 toks/s]
Processed prompts:  10%|▉         | 51/526 [00:04<00:10, 44.80it/s, est. speed input: 838.16 toks/s, output: 738.28 toks/s]
Processed prompts:  11%|█         | 56/526 [00:04<00:10, 45.26it/s, est. speed input: 900.29 toks/s, output: 802.96 toks/s]
Processed prompts:  12%|█▏        | 63/526 [00:04<00:09, 48.93it/s, est. speed input: 994.85 toks/s, output: 897.69 toks/s]
Processed prompts:  13%|█▎        | 71/526 [00:04<00:08, 53.19it/s, est. speed input: 1093.62 toks/s, output: 1005.41 toks/s]
Processed prompts:  15%|█▍        | 77/526 [00:04<00:08, 52.63it/s, est. speed input: 1157.61 toks/s, output: 1081.13 toks/s]
Processed prompts:  16%|█▋        | 86/526 [00:05<00:07, 56.87it/s, est. speed input: 1263.65 toks/s, output: 1199.80 toks/s]
Processed prompts:  17%|█▋        | 92/526 [00:05<00:08, 54.25it/s, est. speed input: 1320.65 toks/s, output: 1271.13 toks/s]
Processed prompts:  19%|█▊        | 98/526 [00:05<00:07, 54.43it/s, est. speed input: 1378.84 toks/s, output: 1344.38 toks/s]
Processed prompts:  20%|██        | 106/526 [00:05<00:07, 56.92it/s, est. speed input: 1456.80 toks/s, output: 1443.99 toks/s]
Processed prompts:  23%|██▎       | 119/526 [00:05<00:06, 67.10it/s, est. speed input: 1595.85 toks/s, output: 1617.73 toks/s]
Processed prompts:  24%|██▍       | 128/526 [00:05<00:05, 67.00it/s, est. speed input: 1679.61 toks/s, output: 1726.36 toks/s]
Processed prompts:  26%|██▋       | 139/526 [00:05<00:05, 70.19it/s, est. speed input: 1779.74 toks/s, output: 1863.19 toks/s]
Processed prompts:  29%|██▊       | 151/526 [00:06<00:05, 74.34it/s, est. speed input: 1889.20 toks/s, output: 2012.88 toks/s]
Processed prompts:  30%|███       | 159/526 [00:06<00:05, 71.07it/s, est. speed input: 1951.03 toks/s, output: 2101.02 toks/s]
Processed prompts:  32%|███▏      | 170/526 [00:06<00:04, 73.34it/s, est. speed input: 2040.36 toks/s, output: 2231.41 toks/s]
Processed prompts:  34%|███▍      | 178/526 [00:06<00:04, 72.79it/s, est. speed input: 2098.28 toks/s, output: 2322.44 toks/s]
Processed prompts:  35%|███▌      | 186/526 [00:06<00:05, 66.49it/s, est. speed input: 2144.50 toks/s, output: 2399.66 toks/s]
Processed prompts:  37%|███▋      | 193/526 [00:06<00:05, 65.16it/s, est. speed input: 2187.27 toks/s, output: 2472.94 toks/s]
Processed prompts:  38%|███▊      | 200/526 [00:06<00:04, 65.26it/s, est. speed input: 2229.60 toks/s, output: 2548.05 toks/s]
Processed prompts:  39%|███▉      | 207/526 [00:06<00:05, 62.75it/s, est. speed input: 2266.63 toks/s, output: 2616.45 toks/s]
Processed prompts:  41%|████      | 214/526 [00:07<00:05, 59.36it/s, est. speed input: 2303.36 toks/s, output: 2679.86 toks/s]
Processed prompts:  42%|████▏     | 220/526 [00:07<00:06, 46.22it/s, est. speed input: 2296.68 toks/s, output: 2671.08 toks/s]
Processed prompts:  43%|████▎     | 226/526 [00:07<00:06, 47.48it/s, est. speed input: 2324.53 toks/s, output: 2708.31 toks/s]
Processed prompts:  44%|████▍     | 232/526 [00:07<00:06, 47.08it/s, est. speed input: 2346.53 toks/s, output: 2750.83 toks/s]
Processed prompts:  45%|████▌     | 238/526 [00:07<00:06, 41.44it/s, est. speed input: 2349.17 toks/s, output: 2763.95 toks/s]
Processed prompts:  46%|████▌     | 243/526 [00:07<00:07, 35.91it/s, est. speed input: 2342.90 toks/s, output: 2772.19 toks/s]
Processed prompts:  47%|████▋     | 247/526 [00:08<00:09, 29.87it/s, est. speed input: 2318.85 toks/s, output: 2742.94 toks/s]
Processed prompts:  48%|████▊     | 251/526 [00:08<00:08, 31.58it/s, est. speed input: 2328.44 toks/s, output: 2774.82 toks/s]
Processed prompts:  48%|████▊     | 255/526 [00:08<00:08, 32.03it/s, est. speed input: 2333.03 toks/s, output: 2793.64 toks/s]
Processed prompts:  49%|████▉     | 259/526 [00:08<00:08, 30.63it/s, est. speed input: 2329.27 toks/s, output: 2775.57 toks/s]
Processed prompts:  50%|█████     | 263/526 [00:08<00:08, 30.63it/s, est. speed input: 2329.12 toks/s, output: 2777.82 toks/s]
Processed prompts:  52%|█████▏    | 272/526 [00:08<00:06, 41.62it/s, est. speed input: 2372.10 toks/s, output: 2817.95 toks/s]
Processed prompts:  53%|█████▎    | 277/526 [00:08<00:06, 39.33it/s, est. speed input: 2376.49 toks/s, output: 2832.93 toks/s]
Processed prompts:  54%|█████▎    | 282/526 [00:08<00:05, 41.01it/s, est. speed input: 2390.20 toks/s, output: 2852.36 toks/s]
Processed prompts:  55%|█████▌    | 290/526 [00:09<00:04, 49.78it/s, est. speed input: 2430.80 toks/s, output: 2882.36 toks/s]
Processed prompts:  56%|█████▋    | 296/526 [00:09<00:04, 47.41it/s, est. speed input: 2444.58 toks/s, output: 2888.38 toks/s]
Processed prompts:  58%|█████▊    | 304/526 [00:09<00:04, 54.98it/s, est. speed input: 2482.65 toks/s, output: 2925.04 toks/s]
Processed prompts:  60%|██████    | 317/526 [00:09<00:02, 74.39it/s, est. speed input: 2562.07 toks/s, output: 3022.39 toks/s]
Processed prompts:  62%|██████▏   | 325/526 [00:09<00:03, 64.73it/s, est. speed input: 2584.30 toks/s, output: 3049.70 toks/s]
Processed prompts:  65%|██████▍   | 340/526 [00:09<00:02, 80.36it/s, est. speed input: 2672.05 toks/s, output: 3142.74 toks/s]
Processed prompts:  67%|██████▋   | 350/526 [00:09<00:02, 80.86it/s, est. speed input: 2715.75 toks/s, output: 3191.68 toks/s]
Processed prompts:  69%|██████▉   | 362/526 [00:09<00:01, 86.98it/s, est. speed input: 2781.05 toks/s, output: 3263.73 toks/s]
Processed prompts:  71%|███████   | 374/526 [00:10<00:01, 92.11it/s, est. speed input: 2843.17 toks/s, output: 3335.16 toks/s]
Processed prompts:  73%|███████▎  | 384/526 [00:10<00:01, 91.58it/s, est. speed input: 2889.27 toks/s, output: 3387.09 toks/s]
Processed prompts:  75%|███████▍  | 394/526 [00:10<00:01, 91.85it/s, est. speed input: 2936.33 toks/s, output: 3450.38 toks/s]
Processed prompts:  77%|███████▋  | 406/526 [00:10<00:01, 94.55it/s, est. speed input: 2994.17 toks/s, output: 3530.36 toks/s]
Processed prompts:  80%|████████  | 421/526 [00:10<00:00, 106.39it/s, est. speed input: 3073.67 toks/s, output: 3650.08 toks/s]
Processed prompts:  83%|████████▎ | 436/526 [00:10<00:00, 117.10it/s, est. speed input: 3154.47 toks/s, output: 3766.54 toks/s]
Processed prompts:  86%|████████▌ | 452/526 [00:10<00:00, 122.81it/s, est. speed input: 3239.29 toks/s, output: 3892.33 toks/s]
Processed prompts:  89%|████████▉ | 468/526 [00:10<00:00, 129.59it/s, est. speed input: 3323.41 toks/s, output: 4016.44 toks/s]
Processed prompts:  92%|█████████▏| 482/526 [00:10<00:00, 120.52it/s, est. speed input: 3383.72 toks/s, output: 4124.65 toks/s]
Processed prompts:  94%|█████████▍| 495/526 [00:11<00:00, 115.57it/s, est. speed input: 3436.49 toks/s, output: 4211.20 toks/s]
Processed prompts:  96%|█████████▋| 507/526 [00:11<00:00, 99.51it/s, est. speed input: 3469.02 toks/s, output: 4279.45 toks/s] 
Processed prompts:  98%|█████████▊| 518/526 [00:11<00:00, 65.14it/s, est. speed input: 3446.64 toks/s, output: 4298.96 toks/s]
Processed prompts: 100%|██████████| 526/526 [00:13<00:00, 38.32it/s, est. speed input: 2968.06 toks/s, output: 3776.81 toks/s]
[rank0]:[W703 18:55:03.116525949 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/nopers/lp_test_output.json
Cleaned temporary files.
Keeping [] in profile
Temp data file saved at data/processed/TEMP_sc_processed_test.json.
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_sc_processed_test.json', '--output_addr', 'data/out/nopers/sc_test_output.json', '--temperature', '0.0', '--top_p', '0.95', '--max_tokens', '4096', '--num_generated_outputs', '1', '--num_contexts', '5', '--max_retries', '5', '--cache_dir', '/work/pi_hzamani_umass_edu/ozel_cache/']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 424 examples [00:00, 7158.90 examples/s]
INFO 07-03 18:55:30 config.py:510] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.
INFO 07-03 18:55:30 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/work/pi_hzamani_umass_edu/ozel_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-03 18:55:32 selector.py:120] Using Flash Attention backend.
INFO 07-03 18:55:33 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-03 18:55:33 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.80it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.68it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.61it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.67it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.67it/s]

INFO 07-03 18:55:36 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-03 18:55:38 worker.py:241] Memory profiling takes 2.08 seconds
INFO 07-03 18:55:38 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-03 18:55:38 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-03 18:55:38 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-03 18:55:38 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-03 18:55:41 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:11,  2.88it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:11,  2.96it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:10,  2.99it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:10,  3.01it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:09,  3.02it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:09,  3.03it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:09,  3.03it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:08,  3.04it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:08,  2.94it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:03<00:08,  2.97it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:03<00:07,  3.00it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:07,  3.02it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:04<00:07,  3.04it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:04<00:06,  3.05it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:04<00:06,  3.06it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:05<00:06,  3.06it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:05<00:05,  3.09it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:05<00:05,  3.11it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:06<00:05,  3.12it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:06<00:04,  3.13it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:06<00:04,  3.14it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:07<00:04,  3.15it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:07<00:03,  3.15it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:07<00:03,  3.16it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:08<00:03,  3.16it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:08<00:02,  3.17it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:08<00:02,  3.17it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:09<00:02,  3.17it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:09<00:01,  3.18it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:09<00:01,  3.19it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:10<00:01,  3.19it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:10<00:00,  3.20it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:10<00:00,  3.14it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:10<00:00,  3.16it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.18it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.10it/s]
INFO 07-03 18:55:53 model_runner.py:1535] Graph capturing finished in 11 secs, took 0.22 GiB
INFO 07-03 18:55:53 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 16.44 seconds
None

Processed prompts:   0%|          | 0/424 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/424 [00:01<12:07,  1.72s/it, est. speed input: 41.28 toks/s, output: 9.88 toks/s]
Processed prompts:   1%|          | 3/424 [00:02<05:32,  1.27it/s, est. speed input: 83.81 toks/s, output: 29.07 toks/s]
Processed prompts:   1%|          | 4/424 [00:02<03:54,  1.79it/s, est. speed input: 104.96 toks/s, output: 44.00 toks/s]
Processed prompts:   1%|          | 5/424 [00:03<03:20,  2.09it/s, est. speed input: 117.61 toks/s, output: 54.76 toks/s]
Processed prompts:   2%|▏         | 7/424 [00:03<01:53,  3.69it/s, est. speed input: 157.65 toks/s, output: 83.37 toks/s]
Processed prompts:   2%|▏         | 8/424 [00:03<01:48,  3.82it/s, est. speed input: 171.13 toks/s, output: 93.74 toks/s]
Processed prompts:   3%|▎         | 12/424 [00:03<00:50,  8.19it/s, est. speed input: 247.06 toks/s, output: 154.52 toks/s]
Processed prompts:   3%|▎         | 14/424 [00:03<00:41,  9.89it/s, est. speed input: 279.59 toks/s, output: 182.27 toks/s]
Processed prompts:   4%|▍         | 18/424 [00:03<00:27, 14.87it/s, est. speed input: 348.48 toks/s, output: 240.94 toks/s]
Processed prompts:   5%|▌         | 22/424 [00:03<00:20, 19.41it/s, est. speed input: 416.01 toks/s, output: 298.34 toks/s]
Processed prompts:   7%|▋         | 29/424 [00:03<00:13, 28.97it/s, est. speed input: 531.95 toks/s, output: 401.54 toks/s]
Processed prompts:   8%|▊         | 33/424 [00:04<00:13, 29.91it/s, est. speed input: 591.57 toks/s, output: 454.34 toks/s]
Processed prompts:   9%|▉         | 39/424 [00:04<00:10, 36.26it/s, est. speed input: 683.85 toks/s, output: 539.72 toks/s]
Processed prompts:  11%|█         | 46/424 [00:04<00:09, 41.69it/s, est. speed input: 782.97 toks/s, output: 636.19 toks/s]
Processed prompts:  12%|█▏        | 51/424 [00:04<00:08, 43.03it/s, est. speed input: 850.74 toks/s, output: 702.32 toks/s]
Processed prompts:  14%|█▎        | 58/424 [00:04<00:07, 46.49it/s, est. speed input: 941.22 toks/s, output: 795.23 toks/s]
Processed prompts:  16%|█▌        | 68/424 [00:04<00:06, 55.01it/s, est. speed input: 1080.05 toks/s, output: 932.78 toks/s]
Processed prompts:  18%|█▊        | 76/424 [00:04<00:06, 57.10it/s, est. speed input: 1185.91 toks/s, output: 1037.00 toks/s]
Processed prompts:  19%|█▉        | 82/424 [00:04<00:06, 55.88it/s, est. speed input: 1255.09 toks/s, output: 1109.80 toks/s]
Processed prompts:  21%|██        | 90/424 [00:05<00:05, 57.68it/s, est. speed input: 1344.43 toks/s, output: 1210.12 toks/s]
Processed prompts:  23%|██▎       | 96/424 [00:05<00:05, 55.88it/s, est. speed input: 1406.68 toks/s, output: 1280.20 toks/s]
Processed prompts:  25%|██▍       | 104/424 [00:05<00:05, 57.69it/s, est. speed input: 1494.76 toks/s, output: 1378.10 toks/s]
Processed prompts:  26%|██▋       | 112/424 [00:05<00:05, 59.23it/s, est. speed input: 1570.12 toks/s, output: 1474.69 toks/s]
Processed prompts:  28%|██▊       | 118/424 [00:05<00:05, 56.59it/s, est. speed input: 1617.45 toks/s, output: 1539.84 toks/s]
Processed prompts:  29%|██▉       | 124/424 [00:05<00:05, 54.73it/s, est. speed input: 1668.30 toks/s, output: 1604.07 toks/s]
Processed prompts:  31%|███       | 131/424 [00:05<00:05, 55.58it/s, est. speed input: 1732.24 toks/s, output: 1683.44 toks/s]
Processed prompts:  32%|███▏      | 137/424 [00:05<00:05, 55.05it/s, est. speed input: 1776.81 toks/s, output: 1749.00 toks/s]
Processed prompts:  34%|███▎      | 143/424 [00:06<00:05, 51.97it/s, est. speed input: 1815.00 toks/s, output: 1808.40 toks/s]
Processed prompts:  35%|███▌      | 149/424 [00:06<00:05, 53.68it/s, est. speed input: 1862.15 toks/s, output: 1876.09 toks/s]
Processed prompts:  37%|███▋      | 155/424 [00:06<00:05, 51.57it/s, est. speed input: 1897.37 toks/s, output: 1935.18 toks/s]
Processed prompts:  38%|███▊      | 163/424 [00:06<00:04, 55.10it/s, est. speed input: 1957.10 toks/s, output: 2023.12 toks/s]
Processed prompts:  40%|████      | 171/424 [00:06<00:04, 59.45it/s, est. speed input: 2018.99 toks/s, output: 2117.53 toks/s]
Processed prompts:  42%|████▏     | 180/424 [00:06<00:03, 66.03it/s, est. speed input: 2094.69 toks/s, output: 2230.01 toks/s]
Processed prompts:  45%|████▌     | 192/424 [00:06<00:02, 79.32it/s, est. speed input: 2203.50 toks/s, output: 2392.25 toks/s]
Processed prompts:  47%|████▋     | 201/424 [00:06<00:02, 81.32it/s, est. speed input: 2273.45 toks/s, output: 2506.47 toks/s]
Processed prompts:  50%|████▉     | 210/424 [00:06<00:02, 76.33it/s, est. speed input: 2329.78 toks/s, output: 2610.11 toks/s]
Processed prompts:  53%|█████▎    | 225/424 [00:07<00:02, 88.81it/s, est. speed input: 2456.14 toks/s, output: 2813.10 toks/s]
Processed prompts:  55%|█████▌    | 234/424 [00:07<00:02, 83.36it/s, est. speed input: 2512.77 toks/s, output: 2920.64 toks/s]
Processed prompts:  57%|█████▋    | 243/424 [00:07<00:02, 80.41it/s, est. speed input: 2569.14 toks/s, output: 3008.36 toks/s]
Processed prompts:  59%|█████▉    | 252/424 [00:07<00:02, 79.23it/s, est. speed input: 2623.67 toks/s, output: 3081.35 toks/s]
Processed prompts:  61%|██████▏   | 260/424 [00:07<00:02, 62.69it/s, est. speed input: 2636.30 toks/s, output: 3101.46 toks/s]
Processed prompts:  64%|██████▎   | 270/424 [00:07<00:02, 69.04it/s, est. speed input: 2701.38 toks/s, output: 3173.22 toks/s]
Processed prompts:  66%|██████▌   | 278/424 [00:07<00:02, 61.76it/s, est. speed input: 2724.35 toks/s, output: 3209.90 toks/s]
Processed prompts:  68%|██████▊   | 287/424 [00:08<00:02, 63.10it/s, est. speed input: 2767.42 toks/s, output: 3250.53 toks/s]
Processed prompts:  70%|███████   | 297/424 [00:08<00:01, 71.01it/s, est. speed input: 2831.84 toks/s, output: 3320.90 toks/s]
Processed prompts:  72%|███████▏  | 306/424 [00:08<00:01, 73.12it/s, est. speed input: 2877.42 toks/s, output: 3384.29 toks/s]
Processed prompts:  75%|███████▍  | 317/424 [00:08<00:01, 80.33it/s, est. speed input: 2942.04 toks/s, output: 3462.18 toks/s]
Processed prompts:  77%|███████▋  | 328/424 [00:08<00:01, 86.38it/s, est. speed input: 3007.22 toks/s, output: 3546.81 toks/s]
Processed prompts:  80%|███████▉  | 338/424 [00:08<00:00, 89.70it/s, est. speed input: 3065.10 toks/s, output: 3624.74 toks/s]
Processed prompts:  82%|████████▏ | 348/424 [00:08<00:00, 88.31it/s, est. speed input: 3116.97 toks/s, output: 3692.74 toks/s]
Processed prompts:  86%|████████▌ | 363/424 [00:08<00:00, 102.15it/s, est. speed input: 3217.71 toks/s, output: 3825.97 toks/s]
Processed prompts:  88%|████████▊ | 374/424 [00:08<00:00, 98.74it/s, est. speed input: 3269.35 toks/s, output: 3912.63 toks/s] 
Processed prompts:  91%|█████████▏| 387/424 [00:09<00:00, 103.45it/s, est. speed input: 3345.06 toks/s, output: 4026.67 toks/s]
Processed prompts:  94%|█████████▍| 398/424 [00:09<00:00, 104.05it/s, est. speed input: 3404.63 toks/s, output: 4133.60 toks/s]
Processed prompts:  96%|█████████▋| 409/424 [00:09<00:00, 87.31it/s, est. speed input: 3432.14 toks/s, output: 4209.25 toks/s] 
Processed prompts:  99%|█████████▉| 419/424 [00:09<00:00, 64.25it/s, est. speed input: 3420.61 toks/s, output: 4251.82 toks/s]
Processed prompts: 100%|██████████| 424/424 [00:11<00:00, 36.67it/s, est. speed input: 2891.76 toks/s, output: 3648.51 toks/s]
{  "personalized_answer": "The sizes of the disks" is the correct phrase to use when referring to multiple disks with different sizes. If you were referring to a single set of disks where the size is uniform, you could use 'the size of the disks'."}
Invalid JSON
{  "personalized_answer": "To legally publish a NES/Famicom ROM wrapped in an emulator app for another platform, you need to ensure that you have the necessary permissions and comply with copyright laws. Here are the steps you should follow:\n\n1. **Obtain Permission**: Contact the copyright holders of the ROMs (Nintendo) and seek permission to distribute them. This is often challenging due to Nintendo's strict policies, but it is the only way to ensure legal compliance.\n\n2. **Comply with Regional Laws**: Different countries have different copyright laws. Ensure that your distribution complies with the laws of the regions where you plan to distribute the app.\n\n3. **Use Legal Emulators**: Use emulators that are known to be legal and have a good reputation. Some emulators are open-source and have a community that supports their legality.\n\n4. **Provide Attribution**: Clearly attribute the ROMs to their original creators and provide a disclaimer about the legal status of the ROMs.\n\n5. **Offer a Clear Warning**: Include a clear warning that the ROMs are provided "as is" and that the user is responsible for any legal consequences of using them.\n\n6. **Consider Legal Advice**: Consult with a lawyer who specializes in copyright law to ensure that your app complies with all relevant laws and regulations.\n\nBy following these steps, you can minimize the risk of legal issues and ensure that your app is as compliant as possible."}
Invalid JSON
None

Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.05it/s, est. speed input: 81.87 toks/s, output: 75.58 toks/s]
Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it, est. speed input: 46.26 toks/s, output: 102.65 toks/s]
Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it, est. speed input: 46.26 toks/s, output: 102.65 toks/s]
[rank0]:[W703 18:56:09.099311459 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/nopers/sc_test_output.json
Cleaned temporary files.
