No conda package cache directories found outside your home directory. To
prevent conda from filling up your home directory, you can create a new
directory at `/work/pi_<your_pi_name>/$USER/.conda/pkgs` and reload the module. 
No conda environment directories found outside your home directory. To prevent
conda from filling up your home directory, you can create a new directory at
`/work/pi_<your_pi_name>/$USER/.conda/envs` and reload the module. 
Loading conda version miniforge3-24.7.1
Loading cuda version 12.6

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:09<02:38,  9.90s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:15<01:51,  7.45s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:21<01:36,  6.91s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:28<01:27,  6.77s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:40<01:42,  8.57s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:47<01:28,  8.01s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:59<01:35,  9.55s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [01:13<01:37, 10.79s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [01:19<01:13,  9.22s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [01:30<01:09,  9.98s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [01:35<00:49,  8.33s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [01:48<00:48,  9.76s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [01:54<00:34,  8.61s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [02:07<00:29,  9.84s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [02:14<00:18,  9.07s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [02:22<00:08,  8.78s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [02:29<00:00,  8.41s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [02:29<00:00,  8.82s/it]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/oyilmazel_umass_edu/LaMP-QA/evaluate_responses.py", line 32, in <module>
[rank0]:     llm = LLM(args.evaluator_llm, download_dir=args.cache_dir, max_model_len=args.max_length, tensor_parallel_size=args.tensor_parallel_size)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/utils.py", line 986, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 230, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 517, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 83, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 266, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 498, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (32000) is larger than the maximum number of tokens that can be stored in KV cache (16448). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W709 14:30:08.551692839 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:09,  1.61it/s]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:06<00:59,  3.95s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:12<01:08,  4.93s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:18<01:08,  5.24s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:19<00:43,  3.60s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:25<00:47,  4.29s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:25<00:31,  3.11s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:26<00:21,  2.34s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:32<00:26,  3.36s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:37<00:28,  4.12s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:42<00:25,  4.29s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:48<00:24,  4.83s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:53<00:19,  4.78s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:53<00:10,  3.55s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:54<00:05,  2.69s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:55<00:02,  2.09s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:56<00:00,  1.67s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:56<00:00,  3.30s/it]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/oyilmazel_umass_edu/LaMP-QA/evaluate_responses.py", line 32, in <module>
[rank0]:     llm = LLM(args.evaluator_llm, download_dir=args.cache_dir, max_model_len=args.max_length, tensor_parallel_size=args.tensor_parallel_size)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/utils.py", line 986, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 230, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 517, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 83, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 266, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 498, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (32000) is larger than the maximum number of tokens that can be stored in KV cache (16448). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W709 14:31:30.989074133 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:05<01:26,  5.38s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:06<00:39,  2.63s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:06<00:24,  1.73s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:07<00:17,  1.32s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:13<00:34,  2.86s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:13<00:23,  2.13s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:19<00:32,  3.25s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:24<00:36,  4.01s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:25<00:23,  2.98s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:26<00:15,  2.27s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:26<00:10,  1.74s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:27<00:07,  1.42s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:29<00:05,  1.49s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:34<00:08,  2.73s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:40<00:07,  3.61s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:46<00:04,  4.23s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:51<00:00,  4.64s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:51<00:00,  3.04s/it]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/oyilmazel_umass_edu/LaMP-QA/evaluate_responses.py", line 32, in <module>
[rank0]:     llm = LLM(args.evaluator_llm, download_dir=args.cache_dir, max_model_len=args.max_length, tensor_parallel_size=args.tensor_parallel_size)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/utils.py", line 986, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 230, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 517, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 83, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 266, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 498, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (32000) is larger than the maximum number of tokens that can be stored in KV cache (16448). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W709 14:32:48.259699563 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:11,  1.37it/s]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:06<00:55,  3.67s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:07<00:36,  2.61s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:09<00:30,  2.33s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:10<00:20,  1.74s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:16<00:33,  3.06s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:16<00:22,  2.29s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:17<00:16,  1.78s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:22<00:23,  2.97s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:28<00:26,  3.80s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:33<00:24,  4.03s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:38<00:22,  4.52s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:43<00:18,  4.57s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:44<00:10,  3.39s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:44<00:05,  2.58s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:45<00:02,  2.02s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:46<00:00,  1.62s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:46<00:00,  2.73s/it]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/oyilmazel_umass_edu/LaMP-QA/evaluate_responses.py", line 32, in <module>
[rank0]:     llm = LLM(args.evaluator_llm, download_dir=args.cache_dir, max_model_len=args.max_length, tensor_parallel_size=args.tensor_parallel_size)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/utils.py", line 986, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 230, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 517, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 83, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 266, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 498, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (32000) is larger than the maximum number of tokens that can be stored in KV cache (16448). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W709 14:34:00.553439350 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:05<01:27,  5.49s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:06<00:40,  2.68s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:11<00:52,  3.78s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:15<00:52,  4.02s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:21<00:54,  4.56s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:21<00:35,  3.25s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:27<00:40,  4.00s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:32<00:40,  4.50s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:33<00:26,  3.31s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:34<00:17,  2.50s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:34<00:11,  1.91s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:35<00:07,  1.54s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:37<00:06,  1.56s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:42<00:08,  2.80s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:48<00:07,  3.66s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:54<00:04,  4.28s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:00<00:00,  4.72s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:00<00:00,  3.53s/it]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/oyilmazel_umass_edu/LaMP-QA/evaluate_responses.py", line 32, in <module>
[rank0]:     llm = LLM(args.evaluator_llm, download_dir=args.cache_dir, max_model_len=args.max_length, tensor_parallel_size=args.tensor_parallel_size)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/utils.py", line 986, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 230, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 517, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 83, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 266, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 498, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (32000) is larger than the maximum number of tokens that can be stored in KV cache (16448). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W709 14:35:28.874543624 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:01<00:27,  1.74s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:02<00:16,  1.09s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:07<00:43,  3.14s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:09<00:30,  2.33s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:09<00:20,  1.74s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:15<00:33,  3.03s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:15<00:22,  2.28s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:16<00:16,  1.78s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:22<00:23,  2.97s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:25<00:20,  2.98s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:29<00:20,  3.44s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:35<00:20,  4.16s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:40<00:17,  4.28s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:40<00:09,  3.20s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:41<00:04,  2.44s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:42<00:01,  1.92s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:42<00:00,  1.55s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:42<00:00,  2.53s/it]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/oyilmazel_umass_edu/LaMP-QA/evaluate_responses.py", line 32, in <module>
[rank0]:     llm = LLM(args.evaluator_llm, download_dir=args.cache_dir, max_model_len=args.max_length, tensor_parallel_size=args.tensor_parallel_size)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/utils.py", line 986, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 230, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 517, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 83, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 266, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/oyilmazel_umass_edu/.conda/envs/lamp/lib/python3.11/site-packages/vllm/worker/worker.py", line 498, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (32000) is larger than the maximum number of tokens that can be stored in KV cache (16448). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
[rank0]:[W709 14:36:39.436442343 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
