Keeping ['ae', 'lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-10 15:48:06.978088_ae_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-10 15:48:06.978088_ae_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-10 15:48:06.978088_ae_processed_test.json', '--batch_size', '4']

  0%|          | 0/102 [00:00<?, ?it/s]
  1%|          | 1/102 [00:00<01:39,  1.02it/s]
  2%|▏         | 2/102 [00:01<01:28,  1.14it/s]
  3%|▎         | 3/102 [00:02<01:24,  1.18it/s]
  4%|▍         | 4/102 [00:03<01:22,  1.19it/s]
  5%|▍         | 5/102 [00:04<01:20,  1.21it/s]
  6%|▌         | 6/102 [00:05<01:18,  1.22it/s]
  9%|▉         | 9/102 [00:07<01:16,  1.21it/s]
 10%|▉         | 10/102 [00:07<01:02,  1.48it/s]
 11%|█         | 11/102 [00:10<01:38,  1.08s/it]
 12%|█▏        | 12/102 [00:12<02:08,  1.42s/it]
 14%|█▎        | 14/102 [00:14<01:58,  1.35s/it]
 16%|█▌        | 16/102 [00:15<01:23,  1.03it/s]
 17%|█▋        | 17/102 [00:16<01:17,  1.10it/s]
 19%|█▊        | 19/102 [00:16<00:48,  1.70it/s]
 21%|██        | 21/102 [00:16<00:32,  2.50it/s]
 23%|██▎       | 23/102 [00:16<00:25,  3.09it/s]
 24%|██▎       | 24/102 [00:17<00:24,  3.24it/s]
 25%|██▍       | 25/102 [00:17<00:22,  3.40it/s]
 25%|██▌       | 26/102 [00:17<00:21,  3.60it/s]
 27%|██▋       | 28/102 [00:17<00:15,  4.77it/s]
 28%|██▊       | 29/102 [00:17<00:13,  5.31it/s]
 30%|███       | 31/102 [00:18<00:10,  7.04it/s]
 31%|███▏      | 32/102 [00:18<00:09,  7.38it/s]
 32%|███▏      | 33/102 [00:18<00:08,  7.69it/s]
 34%|███▍      | 35/102 [00:18<00:09,  7.05it/s]
 35%|███▌      | 36/102 [00:18<00:08,  7.36it/s]
 36%|███▋      | 37/102 [00:18<00:08,  7.43it/s]
 37%|███▋      | 38/102 [00:18<00:08,  7.45it/s]
 38%|███▊      | 39/102 [00:19<00:10,  6.04it/s]
 39%|███▉      | 40/102 [00:19<00:11,  5.26it/s]
 41%|████      | 42/102 [00:19<00:09,  6.06it/s]
 43%|████▎     | 44/102 [00:19<00:07,  7.98it/s]
 44%|████▍     | 45/102 [00:20<00:08,  6.61it/s]
 45%|████▌     | 46/102 [00:20<00:08,  6.90it/s]
 46%|████▌     | 47/102 [00:20<00:07,  7.33it/s]
 47%|████▋     | 48/102 [00:20<00:12,  4.47it/s]
 48%|████▊     | 49/102 [00:21<00:13,  3.82it/s]
 49%|████▉     | 50/102 [00:21<00:16,  3.13it/s]
 50%|█████     | 51/102 [00:22<00:26,  1.90it/s]
 51%|█████     | 52/102 [00:23<00:29,  1.70it/s]
 53%|█████▎    | 54/102 [00:23<00:16,  2.83it/s]
 54%|█████▍    | 55/102 [00:23<00:16,  2.81it/s]
 55%|█████▍    | 56/102 [00:26<00:39,  1.16it/s]
 57%|█████▋    | 58/102 [00:26<00:25,  1.70it/s]
 59%|█████▉    | 60/102 [00:26<00:16,  2.54it/s]
 60%|█████▉    | 61/102 [00:27<00:16,  2.46it/s]
 61%|██████    | 62/102 [00:27<00:14,  2.73it/s]
 62%|██████▏   | 63/102 [00:28<00:18,  2.10it/s]
 64%|██████▎   | 65/102 [00:28<00:11,  3.29it/s]
 65%|██████▍   | 66/102 [00:28<00:11,  3.20it/s]
 66%|██████▌   | 67/102 [00:28<00:09,  3.80it/s]
 68%|██████▊   | 69/102 [00:29<00:06,  5.13it/s]
 69%|██████▊   | 70/102 [00:29<00:07,  4.30it/s]
 70%|██████▉   | 71/102 [00:29<00:08,  3.87it/s]
 72%|███████▏  | 73/102 [00:29<00:05,  5.38it/s]
 74%|███████▎  | 75/102 [00:30<00:03,  6.76it/s]
 75%|███████▍  | 76/102 [00:30<00:04,  5.56it/s]
 76%|███████▋  | 78/102 [00:30<00:03,  6.92it/s]
 78%|███████▊  | 80/102 [00:31<00:03,  5.84it/s]
 79%|███████▉  | 81/102 [00:31<00:03,  6.14it/s]
 81%|████████▏ | 83/102 [00:32<00:04,  3.86it/s]
 82%|████████▏ | 84/102 [00:32<00:06,  2.68it/s]
 83%|████████▎ | 85/102 [00:33<00:06,  2.68it/s]
 84%|████████▍ | 86/102 [00:33<00:05,  2.72it/s]
 85%|████████▌ | 87/102 [00:33<00:04,  3.19it/s]
 86%|████████▋ | 88/102 [00:34<00:04,  3.24it/s]
 87%|████████▋ | 89/102 [00:34<00:03,  3.27it/s]
 88%|████████▊ | 90/102 [00:34<00:03,  3.29it/s]
 90%|█████████ | 92/102 [00:34<00:02,  4.51it/s]
 91%|█████████ | 93/102 [00:35<00:01,  4.63it/s]
 92%|█████████▏| 94/102 [00:35<00:01,  4.60it/s]
 94%|█████████▍| 96/102 [00:35<00:01,  4.65it/s]
 95%|█████████▌| 97/102 [00:36<00:01,  3.37it/s]
 96%|█████████▌| 98/102 [00:36<00:01,  2.75it/s]
 98%|█████████▊| 100/102 [00:37<00:00,  2.63it/s]
 99%|█████████▉| 101/102 [00:37<00:00,  2.69it/s]
100%|██████████| 102/102 [00:38<00:00,  3.15it/s]
100%|██████████| 102/102 [00:38<00:00,  2.67it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-10 15:48:06.978088_ae_processed_test.json', '--output_addr', 'data/out/rag/full_profile/ae_rag_full_profile_test_75_4_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '4', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 102 examples [00:00, 724.98 examples/s]
INFO 07-10 15:49:13 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 07-10 15:49:13 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-10 15:49:14 selector.py:120] Using Flash Attention backend.
INFO 07-10 15:49:16 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-10 15:49:16 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.71it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.68it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.72it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.70it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.70it/s]

INFO 07-10 15:49:19 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-10 15:49:20 worker.py:241] Memory profiling takes 1.04 seconds
INFO 07-10 15:49:20 worker.py:241] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 07-10 15:49:20 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.44GiB.
INFO 07-10 15:49:20 gpu_executor.py:76] # GPU blocks: 61364, # CPU blocks: 4681
INFO 07-10 15:49:20 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 29.96x
INFO 07-10 15:49:23 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:10,  3.30it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:09,  3.58it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:00<00:08,  3.73it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:08,  3.84it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:07,  3.83it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:07,  3.88it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:01<00:07,  3.85it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:06,  3.91it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:02<00:06,  3.88it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:02<00:06,  3.91it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:02<00:06,  3.85it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:06,  3.80it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:03<00:05,  3.79it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:03<00:05,  3.81it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:03<00:05,  3.85it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:04<00:04,  3.85it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:04<00:04,  3.88it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:04<00:04,  3.93it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:04<00:04,  3.89it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:05<00:03,  3.85it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:05<00:03,  3.78it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:05<00:03,  3.83it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:06<00:03,  3.84it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:06<00:02,  3.89it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:06<00:02,  3.89it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:06<00:02,  3.92it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:07<00:02,  3.93it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:07<00:01,  3.89it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:07<00:01,  3.83it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:07<00:01,  3.82it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:08<00:01,  3.89it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:08<00:00,  3.88it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:08<00:00,  3.91it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:08<00:00,  3.91it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.88it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.85it/s]
INFO 07-10 15:49:32 model_runner.py:1535] Graph capturing finished in 9 secs, took 0.30 GiB
INFO 07-10 15:49:32 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 12.83 seconds
None

Processed prompts:   0%|          | 0/102 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/102 [00:04<07:43,  4.59s/it, est. speed input: 302.72 toks/s, output: 23.54 toks/s]
Processed prompts:   3%|▎         | 3/102 [00:04<02:07,  1.28s/it, est. speed input: 766.85 toks/s, output: 70.99 toks/s]
Processed prompts:   5%|▍         | 5/102 [00:05<01:04,  1.50it/s, est. speed input: 1167.00 toks/s, output: 120.21 toks/s]
Processed prompts:  12%|█▏        | 12/102 [00:05<00:17,  5.06it/s, est. speed input: 2283.31 toks/s, output: 305.61 toks/s]
Processed prompts:  19%|█▊        | 19/102 [00:05<00:08,  9.47it/s, est. speed input: 3836.51 toks/s, output: 491.27 toks/s]
Processed prompts:  29%|██▉       | 30/102 [00:05<00:03, 18.30it/s, est. speed input: 5591.56 toks/s, output: 790.45 toks/s]
Processed prompts:  37%|███▋      | 38/102 [00:05<00:02, 25.22it/s, est. speed input: 7617.26 toks/s, output: 1006.40 toks/s]
Processed prompts:  45%|████▌     | 46/102 [00:05<00:01, 32.56it/s, est. speed input: 8812.83 toks/s, output: 1223.22 toks/s]
Processed prompts:  62%|██████▏   | 63/102 [00:05<00:00, 54.29it/s, est. speed input: 12138.20 toks/s, output: 1716.92 toks/s]
Processed prompts:  73%|███████▎  | 74/102 [00:05<00:00, 61.39it/s, est. speed input: 14163.57 toks/s, output: 2025.98 toks/s]
Processed prompts:  83%|████████▎ | 85/102 [00:05<00:00, 67.70it/s, est. speed input: 15871.46 toks/s, output: 2344.32 toks/s]
Processed prompts:  93%|█████████▎| 95/102 [00:06<00:00, 46.13it/s, est. speed input: 17229.50 toks/s, output: 2559.76 toks/s]
Processed prompts: 100%|██████████| 102/102 [00:07<00:00, 12.87it/s, est. speed input: 14611.71 toks/s, output: 2329.13 toks/s]
[rank0]:[W710 15:49:41.939906715 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/full_profile/ae_rag_full_profile_test_75_4_output.json
Cleaned temporary files.
Keeping ['ae', 'lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-10 15:49:43.995402_lp_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-10 15:49:43.995402_lp_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-10 15:49:43.995402_lp_processed_test.json', '--batch_size', '4']

  0%|          | 0/136 [00:00<?, ?it/s]
  1%|          | 1/136 [00:00<00:28,  4.69it/s]
  2%|▏         | 3/136 [00:00<00:15,  8.39it/s]
  3%|▎         | 4/136 [00:01<00:50,  2.62it/s]
  4%|▎         | 5/136 [00:01<00:57,  2.26it/s]
  4%|▍         | 6/136 [00:01<00:44,  2.95it/s]
  6%|▌         | 8/136 [00:02<00:27,  4.72it/s]
  7%|▋         | 10/136 [00:02<00:20,  6.04it/s]
  8%|▊         | 11/136 [00:02<00:25,  4.91it/s]
  9%|▉         | 12/136 [00:02<00:24,  5.06it/s]
 10%|█         | 14/136 [00:02<00:17,  7.04it/s]
 11%|█         | 15/136 [00:03<00:25,  4.72it/s]
 12%|█▏        | 16/136 [00:03<00:35,  3.37it/s]
 12%|█▎        | 17/136 [00:04<00:47,  2.52it/s]
 13%|█▎        | 18/136 [00:04<00:39,  2.98it/s]
 14%|█▍        | 19/136 [00:05<00:46,  2.53it/s]
 15%|█▍        | 20/136 [00:05<00:53,  2.15it/s]
 15%|█▌        | 21/136 [00:06<00:41,  2.78it/s]
 17%|█▋        | 23/136 [00:06<00:25,  4.50it/s]
 18%|█▊        | 25/136 [00:07<00:37,  2.93it/s]
 19%|█▉        | 26/136 [00:07<00:33,  3.25it/s]
 21%|██        | 28/136 [00:07<00:22,  4.74it/s]
 21%|██▏       | 29/136 [00:07<00:26,  4.08it/s]
 22%|██▏       | 30/136 [00:08<00:24,  4.25it/s]
 23%|██▎       | 31/136 [00:08<00:23,  4.46it/s]
 24%|██▎       | 32/136 [00:08<00:33,  3.10it/s]
 25%|██▌       | 34/136 [00:09<00:21,  4.68it/s]
 26%|██▌       | 35/136 [00:09<00:19,  5.26it/s]
 27%|██▋       | 37/136 [00:09<00:14,  6.96it/s]
 28%|██▊       | 38/136 [00:09<00:15,  6.31it/s]
 30%|███       | 41/136 [00:09<00:11,  7.99it/s]
 32%|███▏      | 43/136 [00:09<00:09,  9.45it/s]
 33%|███▎      | 45/136 [00:11<00:22,  3.96it/s]
 35%|███▍      | 47/136 [00:11<00:17,  5.20it/s]
 36%|███▌      | 49/136 [00:12<00:25,  3.45it/s]
 39%|███▉      | 53/136 [00:12<00:14,  5.89it/s]
 40%|████      | 55/136 [00:12<00:15,  5.24it/s]
 42%|████▏     | 57/136 [00:13<00:13,  5.90it/s]
 43%|████▎     | 59/136 [00:13<00:11,  6.65it/s]
 45%|████▍     | 61/136 [00:13<00:11,  6.50it/s]
 47%|████▋     | 64/136 [00:14<00:10,  6.62it/s]
 49%|████▊     | 66/136 [00:14<00:10,  6.73it/s]
 50%|█████     | 68/136 [00:14<00:08,  7.74it/s]
 51%|█████     | 69/136 [00:15<00:13,  5.01it/s]
 51%|█████▏    | 70/136 [00:15<00:15,  4.30it/s]
 52%|█████▏    | 71/136 [00:15<00:17,  3.79it/s]
 53%|█████▎    | 72/136 [00:16<00:22,  2.88it/s]
 54%|█████▎    | 73/136 [00:16<00:20,  3.08it/s]
 55%|█████▌    | 75/136 [00:17<00:19,  3.15it/s]
 57%|█████▋    | 77/136 [00:18<00:22,  2.67it/s]
 57%|█████▋    | 78/136 [00:19<00:27,  2.09it/s]
 58%|█████▊    | 79/136 [00:19<00:32,  1.75it/s]
 59%|█████▉    | 80/136 [00:20<00:36,  1.55it/s]
 60%|█████▉    | 81/136 [00:21<00:38,  1.43it/s]
 60%|██████    | 82/136 [00:22<00:40,  1.33it/s]
 61%|██████    | 83/136 [00:22<00:31,  1.69it/s]
 62%|██████▏   | 84/136 [00:22<00:23,  2.18it/s]
 62%|██████▎   | 85/136 [00:23<00:29,  1.70it/s]
 63%|██████▎   | 86/136 [00:24<00:33,  1.48it/s]
 64%|██████▍   | 87/136 [00:25<00:31,  1.57it/s]
 65%|██████▍   | 88/136 [00:25<00:29,  1.65it/s]
 66%|██████▌   | 90/136 [00:26<00:18,  2.52it/s]
 68%|██████▊   | 93/136 [00:26<00:11,  3.72it/s]
 69%|██████▉   | 94/136 [00:26<00:12,  3.31it/s]
 70%|██████▉   | 95/136 [00:27<00:14,  2.79it/s]
 71%|███████   | 96/136 [00:28<00:16,  2.37it/s]
 72%|███████▏  | 98/136 [00:28<00:10,  3.58it/s]
 74%|███████▎  | 100/136 [00:28<00:07,  4.85it/s]
 75%|███████▌  | 102/136 [00:29<00:08,  4.00it/s]
 76%|███████▋  | 104/136 [00:29<00:06,  4.97it/s]
 78%|███████▊  | 106/136 [00:29<00:04,  6.41it/s]
 79%|███████▉  | 108/136 [00:30<00:07,  3.57it/s]
 80%|████████  | 109/136 [00:32<00:17,  1.54it/s]
 82%|████████▏ | 111/136 [00:33<00:13,  1.84it/s]
 82%|████████▏ | 112/136 [00:33<00:11,  2.18it/s]
 83%|████████▎ | 113/136 [00:34<00:11,  2.08it/s]
 84%|████████▍ | 114/136 [00:34<00:09,  2.35it/s]
 85%|████████▍ | 115/136 [00:34<00:07,  2.66it/s]
 86%|████████▌ | 117/136 [00:34<00:04,  4.14it/s]
 87%|████████▋ | 118/136 [00:35<00:04,  3.82it/s]
 88%|████████▊ | 119/136 [00:35<00:03,  4.36it/s]
 88%|████████▊ | 120/136 [00:35<00:04,  3.90it/s]
 90%|████████▉ | 122/136 [00:35<00:02,  4.73it/s]
 91%|█████████ | 124/136 [00:36<00:02,  4.07it/s]
 93%|█████████▎| 126/136 [00:36<00:02,  3.87it/s]
 93%|█████████▎| 127/136 [00:37<00:03,  2.60it/s]
 94%|█████████▍| 128/136 [00:37<00:02,  3.10it/s]
 96%|█████████▋| 131/136 [00:40<00:02,  1.81it/s]
 98%|█████████▊| 133/136 [00:40<00:01,  2.41it/s]
 99%|█████████▉| 135/136 [00:41<00:00,  2.53it/s]
100%|██████████| 136/136 [00:41<00:00,  2.35it/s]
100%|██████████| 136/136 [00:41<00:00,  3.25it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-10 15:49:43.995402_lp_processed_test.json', '--output_addr', 'data/out/rag/full_profile/lp_rag_full_profile_test_75_4_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '4', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 136 examples [00:00, 1047.87 examples/s]
INFO 07-10 15:50:50 config.py:510] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 07-10 15:50:50 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-10 15:50:52 selector.py:120] Using Flash Attention backend.
INFO 07-10 15:50:53 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-10 15:50:53 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.69it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.69it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.74it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.72it/s]

INFO 07-10 15:50:56 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-10 15:50:57 worker.py:241] Memory profiling takes 1.03 seconds
INFO 07-10 15:50:57 worker.py:241] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 07-10 15:50:57 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.44GiB.
INFO 07-10 15:50:57 gpu_executor.py:76] # GPU blocks: 61364, # CPU blocks: 4681
INFO 07-10 15:50:57 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 29.96x
INFO 07-10 15:51:00 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:10,  3.36it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:09,  3.64it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:00<00:08,  3.71it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:08,  3.80it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:07,  3.75it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:07,  3.83it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:01<00:07,  3.80it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:07,  3.85it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:02<00:06,  3.83it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:02<00:06,  3.76it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:02<00:06,  3.82it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:06,  3.81it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:03<00:05,  3.86it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:03<00:05,  3.68it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:04<00:10,  1.90it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:05<00:08,  2.25it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:05<00:07,  2.53it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:05<00:05,  2.86it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:05<00:05,  3.09it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:06<00:04,  3.25it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:06<00:04,  3.41it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:06<00:03,  3.57it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:06<00:03,  3.69it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:07<00:02,  3.74it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:07<00:02,  3.85it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:07<00:02,  3.83it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:07<00:02,  3.89it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:08<00:01,  3.89it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:08<00:01,  3.81it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:08<00:01,  3.83it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:08<00:01,  3.87it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:09<00:00,  3.90it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:09<00:00,  3.91it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:09<00:00,  3.97it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.88it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.50it/s]
INFO 07-10 15:51:10 model_runner.py:1535] Graph capturing finished in 10 secs, took 0.30 GiB
INFO 07-10 15:51:10 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 13.76 seconds
None

Processed prompts:   0%|          | 0/136 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/136 [00:05<12:23,  5.51s/it, est. speed input: 120.89 toks/s, output: 19.06 toks/s]
Processed prompts:   1%|▏         | 2/136 [00:05<05:20,  2.39s/it, est. speed input: 242.60 toks/s, output: 38.45 toks/s]
Processed prompts:   4%|▎         | 5/136 [00:05<01:32,  1.42it/s, est. speed input: 719.46 toks/s, output: 98.69 toks/s]
Processed prompts:   5%|▌         | 7/136 [00:05<00:56,  2.27it/s, est. speed input: 931.82 toks/s, output: 138.30 toks/s]
Processed prompts:   7%|▋         | 9/136 [00:06<00:44,  2.85it/s, est. speed input: 1198.29 toks/s, output: 170.53 toks/s]
Processed prompts:  11%|█         | 15/136 [00:06<00:18,  6.66it/s, est. speed input: 1976.78 toks/s, output: 291.71 toks/s]
Processed prompts:  20%|█▉        | 27/136 [00:06<00:06, 16.23it/s, est. speed input: 3689.58 toks/s, output: 542.76 toks/s]
Processed prompts:  29%|██▊       | 39/136 [00:06<00:03, 27.40it/s, est. speed input: 5670.76 toks/s, output: 799.57 toks/s]
Processed prompts:  36%|███▌      | 49/136 [00:06<00:02, 35.53it/s, est. speed input: 6839.02 toks/s, output: 1010.12 toks/s]
Processed prompts:  46%|████▋     | 63/136 [00:06<00:01, 50.88it/s, est. speed input: 9188.30 toks/s, output: 1321.07 toks/s]
Processed prompts:  54%|█████▎    | 73/136 [00:07<00:01, 59.65it/s, est. speed input: 10445.00 toks/s, output: 1541.04 toks/s]
Processed prompts:  64%|██████▍   | 87/136 [00:07<00:00, 75.02it/s, est. speed input: 12332.26 toks/s, output: 1865.68 toks/s]
Processed prompts:  75%|███████▌  | 102/136 [00:07<00:00, 90.46it/s, est. speed input: 14412.69 toks/s, output: 2222.04 toks/s]
Processed prompts:  84%|████████▍ | 114/136 [00:07<00:00, 84.63it/s, est. speed input: 15953.85 toks/s, output: 2490.11 toks/s]
Processed prompts:  92%|█████████▏| 125/136 [00:07<00:00, 83.12it/s, est. speed input: 17142.90 toks/s, output: 2755.01 toks/s]
Processed prompts:  99%|█████████▉| 135/136 [00:08<00:00, 22.49it/s, est. speed input: 15952.81 toks/s, output: 2690.45 toks/s]
Processed prompts: 100%|██████████| 136/136 [00:09<00:00, 13.78it/s, est. speed input: 14461.60 toks/s, output: 2476.59 toks/s]
[rank0]:[W710 15:51:21.855116850 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/full_profile/lp_rag_full_profile_test_75_4_output.json
Cleaned temporary files.
Keeping ['ae', 'lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-10 15:51:24.326032_sc_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-10 15:51:24.326032_sc_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-10 15:51:24.326032_sc_processed_test.json', '--batch_size', '4']

  0%|          | 0/106 [00:00<?, ?it/s]
  1%|          | 1/106 [00:00<00:30,  3.44it/s]
  3%|▎         | 3/106 [00:00<00:15,  6.66it/s]
  4%|▍         | 4/106 [00:00<00:22,  4.52it/s]
  5%|▍         | 5/106 [00:01<00:20,  4.85it/s]
  6%|▌         | 6/106 [00:01<00:17,  5.72it/s]
  7%|▋         | 7/106 [00:01<00:23,  4.29it/s]
  8%|▊         | 8/106 [00:02<00:43,  2.23it/s]
  8%|▊         | 9/106 [00:02<00:33,  2.90it/s]
 10%|█         | 11/106 [00:02<00:20,  4.56it/s]
 14%|█▍        | 15/106 [00:02<00:10,  8.83it/s]
 16%|█▌        | 17/106 [00:03<00:12,  7.38it/s]
 18%|█▊        | 19/106 [00:03<00:09,  8.90it/s]
 20%|█▉        | 21/106 [00:03<00:14,  5.83it/s]
 22%|██▏       | 23/106 [00:04<00:17,  4.65it/s]
 23%|██▎       | 24/106 [00:04<00:19,  4.18it/s]
 24%|██▎       | 25/106 [00:05<00:17,  4.56it/s]
 25%|██▌       | 27/106 [00:05<00:17,  4.54it/s]
 27%|██▋       | 29/106 [00:05<00:12,  5.94it/s]
 29%|██▉       | 31/106 [00:05<00:10,  7.31it/s]
 31%|███       | 33/106 [00:05<00:08,  8.79it/s]
 33%|███▎      | 35/106 [00:06<00:08,  7.97it/s]
 35%|███▍      | 37/106 [00:09<00:40,  1.69it/s]
 38%|███▊      | 40/106 [00:09<00:25,  2.61it/s]
 39%|███▊      | 41/106 [00:10<00:25,  2.55it/s]
 40%|███▉      | 42/106 [00:10<00:21,  2.94it/s]
 41%|████      | 43/106 [00:10<00:18,  3.40it/s]
 42%|████▏     | 45/106 [00:10<00:12,  4.88it/s]
 44%|████▍     | 47/106 [00:11<00:14,  3.94it/s]
 45%|████▌     | 48/106 [00:11<00:13,  4.27it/s]
 46%|████▌     | 49/106 [00:11<00:17,  3.29it/s]
 47%|████▋     | 50/106 [00:12<00:15,  3.70it/s]
 49%|████▉     | 52/106 [00:12<00:09,  5.41it/s]
 50%|█████     | 53/106 [00:12<00:14,  3.73it/s]
 51%|█████     | 54/106 [00:12<00:13,  3.92it/s]
 52%|█████▏    | 55/106 [00:13<00:17,  2.99it/s]
 53%|█████▎    | 56/106 [00:13<00:15,  3.29it/s]
 54%|█████▍    | 57/106 [00:13<00:12,  3.99it/s]
 55%|█████▍    | 58/106 [00:14<00:11,  4.06it/s]
 56%|█████▌    | 59/106 [00:14<00:15,  2.97it/s]
 58%|█████▊    | 61/106 [00:15<00:12,  3.66it/s]
 59%|█████▉    | 63/106 [00:15<00:08,  5.15it/s]
 62%|██████▏   | 66/106 [00:15<00:05,  7.48it/s]
 65%|██████▌   | 69/106 [00:15<00:03,  9.86it/s]
 67%|██████▋   | 71/106 [00:16<00:06,  5.81it/s]
 68%|██████▊   | 72/106 [00:16<00:05,  6.03it/s]
 69%|██████▉   | 73/106 [00:16<00:05,  6.44it/s]
 70%|██████▉   | 74/106 [00:17<00:10,  3.17it/s]
 71%|███████   | 75/106 [00:17<00:09,  3.35it/s]
 72%|███████▏  | 76/106 [00:17<00:08,  3.44it/s]
 73%|███████▎  | 77/106 [00:18<00:09,  3.12it/s]
 75%|███████▍  | 79/106 [00:18<00:07,  3.70it/s]
 75%|███████▌  | 80/106 [00:19<00:09,  2.86it/s]
 77%|███████▋  | 82/106 [00:19<00:08,  2.89it/s]
 78%|███████▊  | 83/106 [00:20<00:09,  2.51it/s]
 79%|███████▉  | 84/106 [00:20<00:07,  3.02it/s]
 80%|████████  | 85/106 [00:20<00:06,  3.35it/s]
 82%|████████▏ | 87/106 [00:21<00:03,  4.89it/s]
 83%|████████▎ | 88/106 [00:21<00:03,  4.84it/s]
 85%|████████▍ | 90/106 [00:21<00:03,  4.62it/s]
 86%|████████▌ | 91/106 [00:22<00:03,  4.07it/s]
 88%|████████▊ | 93/106 [00:22<00:02,  5.37it/s]
 89%|████████▊ | 94/106 [00:22<00:02,  5.84it/s]
 91%|█████████ | 96/106 [00:22<00:01,  7.42it/s]
 92%|█████████▏| 97/106 [00:22<00:01,  5.18it/s]
 92%|█████████▏| 98/106 [00:23<00:01,  4.18it/s]
 93%|█████████▎| 99/106 [00:24<00:02,  2.76it/s]
 94%|█████████▍| 100/106 [00:24<00:02,  2.19it/s]
 95%|█████████▌| 101/106 [00:24<00:01,  2.70it/s]
 96%|█████████▌| 102/106 [00:25<00:01,  3.33it/s]
 97%|█████████▋| 103/106 [00:25<00:01,  2.37it/s]
 99%|█████████▉| 105/106 [00:26<00:00,  2.34it/s]
100%|██████████| 106/106 [00:27<00:00,  2.40it/s]
100%|██████████| 106/106 [00:27<00:00,  3.93it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-10 15:51:24.326032_sc_processed_test.json', '--output_addr', 'data/out/rag/full_profile/sc_rag_full_profile_test_75_4_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '4', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 106 examples [00:00, 1091.17 examples/s]
INFO 07-10 15:52:15 config.py:510] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 07-10 15:52:15 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-10 15:52:17 selector.py:120] Using Flash Attention backend.
INFO 07-10 15:52:19 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-10 15:52:19 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.69it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.69it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.75it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

INFO 07-10 15:52:22 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-10 15:52:23 worker.py:241] Memory profiling takes 1.04 seconds
INFO 07-10 15:52:23 worker.py:241] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 07-10 15:52:23 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.44GiB.
INFO 07-10 15:52:23 gpu_executor.py:76] # GPU blocks: 61364, # CPU blocks: 4681
INFO 07-10 15:52:23 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 29.96x
INFO 07-10 15:52:25 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:10,  3.37it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:09,  3.63it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:00<00:08,  3.78it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:08,  3.84it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:07,  3.85it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:07,  3.82it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:01<00:07,  3.78it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:06,  3.86it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:02<00:06,  3.85it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:02<00:06,  3.89it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:02<00:06,  3.86it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:05,  3.88it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:03<00:05,  3.89it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:03<00:05,  3.77it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:03<00:05,  3.83it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:04<00:04,  3.88it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:04<00:04,  3.87it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:04<00:04,  3.92it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:04<00:04,  3.89it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:05<00:03,  3.94it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:05<00:03,  3.92it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:05<00:03,  3.92it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:05<00:03,  3.90it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:06<00:02,  3.93it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:06<00:02,  3.98it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:06<00:02,  3.94it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:06<00:02,  3.91it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:07<00:01,  3.88it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:07<00:01,  3.82it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:07<00:01,  3.84it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:08<00:01,  3.89it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:08<00:00,  3.89it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:08<00:00,  3.92it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:08<00:00,  3.96it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.77it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.86it/s]
INFO 07-10 15:52:34 model_runner.py:1535] Graph capturing finished in 9 secs, took 0.30 GiB
INFO 07-10 15:52:34 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 12.80 seconds
None

Processed prompts:   0%|          | 0/106 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/106 [00:04<07:42,  4.40s/it, est. speed input: 219.93 toks/s, output: 21.81 toks/s]
Processed prompts:   2%|▏         | 2/106 [00:04<03:22,  1.94s/it, est. speed input: 350.20 toks/s, output: 44.32 toks/s]
Processed prompts:   3%|▎         | 3/106 [00:04<01:56,  1.14s/it, est. speed input: 459.76 toks/s, output: 67.53 toks/s]
Processed prompts:   7%|▋         | 7/106 [00:04<00:33,  2.95it/s, est. speed input: 1560.74 toks/s, output: 165.35 toks/s]
Processed prompts:  10%|█         | 11/106 [00:05<00:17,  5.44it/s, est. speed input: 2334.86 toks/s, output: 264.73 toks/s]
Processed prompts:  15%|█▌        | 16/106 [00:05<00:09,  9.18it/s, est. speed input: 3602.61 toks/s, output: 391.12 toks/s]
Processed prompts:  25%|██▌       | 27/106 [00:05<00:03, 20.14it/s, est. speed input: 5478.53 toks/s, output: 684.30 toks/s]
Processed prompts:  32%|███▏      | 34/106 [00:05<00:02, 26.67it/s, est. speed input: 6633.24 toks/s, output: 869.91 toks/s]
Processed prompts:  41%|████      | 43/106 [00:05<00:01, 36.80it/s, est. speed input: 8265.67 toks/s, output: 1117.46 toks/s]
Processed prompts:  51%|█████     | 54/106 [00:05<00:01, 49.87it/s, est. speed input: 10123.28 toks/s, output: 1425.43 toks/s]
Processed prompts:  63%|██████▎   | 67/106 [00:05<00:00, 65.45it/s, est. speed input: 12847.26 toks/s, output: 1797.96 toks/s]
Processed prompts:  73%|███████▎  | 77/106 [00:05<00:00, 68.49it/s, est. speed input: 14320.47 toks/s, output: 2076.77 toks/s]
Processed prompts:  81%|████████  | 86/106 [00:06<00:00, 67.43it/s, est. speed input: 15834.01 toks/s, output: 2328.17 toks/s]
Processed prompts:  92%|█████████▏| 97/106 [00:06<00:00, 75.83it/s, est. speed input: 17594.02 toks/s, output: 2670.69 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:07<00:00, 17.64it/s, est. speed input: 15629.53 toks/s, output: 2462.81 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:07<00:00, 13.89it/s, est. speed input: 15629.53 toks/s, output: 2462.81 toks/s]
{  "personalized_answer": {    "Based on your past questions and preferences, it seems that you value clarity, elegance, and the impact of your work. When considering the referee's suggestion for a better proof of your theorem, it would be best to include both proofs in your paper. Present your original proof first, as it provides a clear and accessible path for understanding the theorem. Then, introduce the improved proof, highlighting its advantages and how it enhances the overall understanding of the result. This approach respects the aesthetic and practical aspects of your work, aligning with your past concerns about the value and presentation of mathematical results. Additionally, including both proofs demonstrates your thoroughness and openness to improvement, which can strengthen your paper and its reception in the academic community."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 787.31 toks/s, output: 125.08 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 787.31 toks/s, output: 125.08 toks/s]
{  "personalized_answer": {    "Based on your past questions and preferences, it seems that you value clarity, elegance, and the impact of your work. When deciding whether to publish your original proof or the better proof suggested by the referee, it would be best to include both. This approach allows you to acknowledge the referee's contribution while also showcasing the improved clarity and elegance of the new proof. Additionally, by including both proofs, you provide readers with a comprehensive understanding of the theorem and its development, which aligns with your preference for thoroughness and rigorous presentation. This way, you can ensure that your paper is both academically robust and accessible to a wide audience."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it, est. speed input: 647.27 toks/s, output: 125.37 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it, est. speed input: 647.27 toks/s, output: 125.37 toks/s]
{  "personalized_answer": {    "Based on your past questions and preferences, it seems that you value clarity, elegance, and the impact of your work. When a referee suggests a better proof for your theorem, it would be wise to consider both the original and the improved proof. Here’s a suggestion that aligns with your past concerns: Publish both proofs in your paper. You can present the original proof first, as it might be more accessible and easier to understand for a broader audience. Then, you can include the improved proof in a subsequent section, highlighting its advantages and how it enhances the understanding of the theorem. This approach respects the clarity and accessibility you value, while also showcasing the elegance and rigor of the improved proof. Additionally, it provides a comprehensive view of the theorem, which could be beneficial for future research and applications."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 686.31 toks/s, output: 125.46 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 686.31 toks/s, output: 125.46 toks/s]
{  "personalized_answer": {    "Based on your previous questions and preferences, it seems that you value clarity, elegance, and the impact of your work. Given the referee's suggestion of a better proof for your theorem, it would be wise to include both proofs in your paper. Present your original proof first, as it provides a clear and accessible pathway for understanding the theorem. Then, introduce the improved proof, highlighting the advantages and how it enhances the original result. This approach respects the importance of transparency and the potential for your work to be more impactful. Additionally, including both proofs demonstrates your commitment to rigorous and comprehensive research, which aligns with the values of a mathematics journal. This way, you can satisfy both the clarity you value and the need to present the most effective proof."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it, est. speed input: 590.55 toks/s, output: 125.95 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it, est. speed input: 590.55 toks/s, output: 125.95 toks/s]
{  "personalized_answer": {    "Based on your past questions and the detailed descriptions, it seems you value clarity, practicality, and the elegance of your work. When deciding whether to publish your original proof or the better proof suggested by the referee, it's important to consider the purpose and audience of your paper. Since your work is in mathematics and you value elegant and clear proofs, it would be beneficial to include both proofs. You could present the original proof first, as it might be more accessible to a broader audience, and then follow it with the improved proof. This approach respects the originality of your work while also providing a more robust and rigorous version. This way, your paper will be appreciated for its elegance and for offering a thorough understanding of the theorem. Additionally, you could mention in the introduction or conclusion the improvements suggested by the referee, acknowledging their contribution and the enhanced clarity and rigor of the final proof."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, est. speed input: 661.11 toks/s, output: 125.89 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, est. speed input: 661.11 toks/s, output: 125.89 toks/s]
{  "personalized_answer": {    "Based on your past questions and the detailed descriptions, it seems that you are very particular about the clarity and elegance of your proofs, as well as their practical implications. When a referee suggests a better proof for your theorem, it's generally a good idea to consider both the quality and the impact of the proof. Since the referee's suggestion is likely to improve the paper, it would be wise to incorporate the better proof. This not only enhances the rigor and elegance of your paper but also aligns with the high standards of mathematical journals. However, it might be beneficial to include a brief note explaining the improvements made and how they enhance the original proof, ensuring that the paper remains transparent and credible. This approach respects the originality of your work while acknowledging the valuable feedback from the referee."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 657.82 toks/s, output: 125.98 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 657.82 toks/s, output: 125.98 toks/s]
{  "personalized_answer": {    "Based on your previous questions and interests, it seems that you value both the clarity and practical impact of your research. Given that a referee has suggested a better proof for your theorem, it would be wise to consider both the elegance and the rigor of the new proof. Since you mentioned in your past questions that a more rigorous and general approach is generally appreciated, even if it might be more challenging to understand, it might be beneficial to include the better proof in your paper. However, to maintain the readability and accessibility of your paper, you could consider presenting the new proof as the primary one and providing a section that outlines the original proof, highlighting the improvements and the rationale behind the new approach. This way, you ensure that your paper is both thorough and easy to follow for a broader audience."  }}
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 720.67 toks/s, output: 125.47 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 720.67 toks/s, output: 125.47 toks/s]
[rank0]:[W710 15:52:53.046552407 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/full_profile/sc_rag_full_profile_test_75_4_output.json
Cleaned temporary files.
Keeping ['lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-10 15:52:56.236119_ae_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-10 15:52:56.236119_ae_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-10 15:52:56.236119_ae_processed_test.json', '--batch_size', '4']

  0%|          | 0/102 [00:00<?, ?it/s]
  1%|          | 1/102 [00:00<01:06,  1.53it/s]
  2%|▏         | 2/102 [00:01<00:55,  1.80it/s]
  3%|▎         | 3/102 [00:01<00:51,  1.92it/s]
  4%|▍         | 4/102 [00:02<00:49,  1.97it/s]
  5%|▍         | 5/102 [00:02<00:48,  2.01it/s]
  6%|▌         | 6/102 [00:03<00:47,  2.04it/s]
  9%|▉         | 9/102 [00:04<00:47,  1.97it/s]
 11%|█         | 11/102 [00:06<00:54,  1.66it/s]
 12%|█▏        | 12/102 [00:07<01:10,  1.27it/s]
 14%|█▎        | 14/102 [00:09<01:08,  1.28it/s]
 16%|█▌        | 16/102 [00:09<00:50,  1.70it/s]
 17%|█▋        | 17/102 [00:10<00:46,  1.82it/s]
 20%|█▉        | 20/102 [00:10<00:26,  3.15it/s]
 22%|██▏       | 22/102 [00:10<00:19,  4.09it/s]
 24%|██▎       | 24/102 [00:10<00:16,  4.71it/s]
 25%|██▍       | 25/102 [00:10<00:15,  4.98it/s]
 26%|██▋       | 27/102 [00:10<00:11,  6.53it/s]
 28%|██▊       | 29/102 [00:11<00:09,  7.57it/s]
 31%|███▏      | 32/102 [00:11<00:06, 10.66it/s]
 34%|███▍      | 35/102 [00:11<00:06, 11.13it/s]
 36%|███▋      | 37/102 [00:11<00:05, 12.31it/s]
 38%|███▊      | 39/102 [00:11<00:05, 10.67it/s]
 40%|████      | 41/102 [00:11<00:05, 11.23it/s]
 42%|████▏     | 43/102 [00:12<00:05, 11.56it/s]
 44%|████▍     | 45/102 [00:12<00:05, 10.87it/s]
 46%|████▌     | 47/102 [00:12<00:04, 11.54it/s]
 48%|████▊     | 49/102 [00:12<00:07,  7.30it/s]
 49%|████▉     | 50/102 [00:13<00:08,  5.84it/s]
 50%|█████     | 51/102 [00:14<00:15,  3.32it/s]
 51%|█████     | 52/102 [00:14<00:16,  3.04it/s]
 53%|█████▎    | 54/102 [00:14<00:10,  4.46it/s]
 54%|█████▍    | 55/102 [00:14<00:10,  4.67it/s]
 55%|█████▍    | 56/102 [00:16<00:28,  1.61it/s]
 57%|█████▋    | 58/102 [00:17<00:18,  2.39it/s]
 60%|█████▉    | 61/102 [00:17<00:11,  3.45it/s]
 61%|██████    | 62/102 [00:17<00:10,  3.65it/s]
 62%|██████▏   | 63/102 [00:18<00:12,  3.09it/s]
 65%|██████▍   | 66/102 [00:18<00:07,  4.51it/s]
 68%|██████▊   | 69/102 [00:18<00:04,  6.75it/s]
 70%|██████▉   | 71/102 [00:19<00:05,  6.00it/s]
 73%|███████▎  | 74/102 [00:19<00:03,  8.42it/s]
 75%|███████▍  | 76/102 [00:19<00:03,  7.92it/s]
 76%|███████▋  | 78/102 [00:19<00:02,  9.23it/s]
 78%|███████▊  | 80/102 [00:19<00:02,  8.93it/s]
 80%|████████  | 82/102 [00:19<00:01, 10.30it/s]
 82%|████████▏ | 84/102 [00:20<00:03,  5.10it/s]
 84%|████████▍ | 86/102 [00:21<00:03,  4.78it/s]
 86%|████████▋ | 88/102 [00:21<00:02,  5.56it/s]
 87%|████████▋ | 89/102 [00:21<00:02,  5.91it/s]
 88%|████████▊ | 90/102 [00:21<00:01,  6.27it/s]
 90%|█████████ | 92/102 [00:21<00:01,  8.23it/s]
 92%|█████████▏| 94/102 [00:22<00:00,  8.29it/s]
 94%|█████████▍| 96/102 [00:22<00:00,  8.88it/s]
 96%|█████████▌| 98/102 [00:23<00:00,  4.55it/s]
 98%|█████████▊| 100/102 [00:23<00:00,  4.45it/s]
 99%|█████████▉| 101/102 [00:23<00:00,  4.49it/s]
100%|██████████| 102/102 [00:23<00:00,  4.88it/s]
100%|██████████| 102/102 [00:23<00:00,  4.25it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-10 15:52:56.236119_ae_processed_test.json', '--output_addr', 'data/out/rag/asym/ae_rag_lp_sc_test_75_4_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '4', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 102 examples [00:00, 1467.35 examples/s]
INFO 07-10 15:53:43 config.py:510] This model supports multiple tasks: {'reward', 'embed', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 07-10 15:53:43 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-10 15:53:44 selector.py:120] Using Flash Attention backend.
INFO 07-10 15:53:46 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-10 15:53:46 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.71it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.70it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.76it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.74it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

INFO 07-10 15:53:49 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-10 15:53:50 worker.py:241] Memory profiling takes 1.03 seconds
INFO 07-10 15:53:50 worker.py:241] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 07-10 15:53:50 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.44GiB.
INFO 07-10 15:53:50 gpu_executor.py:76] # GPU blocks: 61364, # CPU blocks: 4681
INFO 07-10 15:53:50 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 29.96x
INFO 07-10 15:53:52 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:10,  3.36it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:08,  3.71it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:00<00:08,  3.74it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:08,  3.86it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:07,  3.80it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:08,  3.36it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:01<00:08,  3.48it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:07,  3.64it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:02<00:07,  3.69it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:02<00:06,  3.77it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:02<00:06,  3.79it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:05,  3.83it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:03<00:05,  3.79it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:03<00:05,  3.74it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:04<00:05,  3.74it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:04<00:05,  3.75it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:04<00:04,  3.79it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:04<00:04,  3.82it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:05<00:04,  3.82it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:05<00:03,  3.78it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:05<00:03,  3.73it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:05<00:03,  3.70it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:06<00:03,  3.70it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:06<00:02,  3.77it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:06<00:02,  3.79it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:06<00:02,  3.83it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:07<00:02,  3.81it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:07<00:01,  3.84it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:07<00:01,  3.71it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:08<00:01,  3.76it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:08<00:01,  3.81it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:08<00:00,  3.78it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:08<00:00,  3.76it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:09<00:00,  3.75it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.78it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.75it/s]
INFO 07-10 15:54:02 model_runner.py:1535] Graph capturing finished in 9 secs, took 0.30 GiB
INFO 07-10 15:54:02 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 13.07 seconds
None

Processed prompts:   0%|          | 0/102 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/102 [00:04<07:58,  4.74s/it, est. speed input: 337.75 toks/s, output: 23.01 toks/s]
Processed prompts:   3%|▎         | 3/102 [00:04<02:05,  1.27s/it, est. speed input: 898.52 toks/s, output: 69.39 toks/s]
Processed prompts:   7%|▋         | 7/102 [00:04<00:40,  2.35it/s, est. speed input: 1875.60 toks/s, output: 165.37 toks/s]
Processed prompts:  13%|█▎        | 13/102 [00:05<00:16,  5.41it/s, est. speed input: 2987.45 toks/s, output: 311.75 toks/s]
Processed prompts:  18%|█▊        | 18/102 [00:05<00:09,  8.50it/s, est. speed input: 4000.01 toks/s, output: 432.23 toks/s]
Processed prompts:  27%|██▋       | 28/102 [00:05<00:04, 16.49it/s, est. speed input: 6058.45 toks/s, output: 684.06 toks/s]
Processed prompts:  33%|███▎      | 34/102 [00:05<00:03, 21.30it/s, est. speed input: 6837.14 toks/s, output: 835.34 toks/s]
Processed prompts:  43%|████▎     | 44/102 [00:05<00:01, 31.74it/s, est. speed input: 8912.89 toks/s, output: 1099.63 toks/s]
Processed prompts:  59%|█████▉    | 60/102 [00:05<00:00, 51.56it/s, est. speed input: 11812.18 toks/s, output: 1541.09 toks/s]
Processed prompts:  70%|██████▉   | 71/102 [00:05<00:00, 59.95it/s, est. speed input: 14273.30 toks/s, output: 1837.46 toks/s]
Processed prompts:  78%|███████▊  | 80/102 [00:05<00:00, 60.71it/s, est. speed input: 15917.49 toks/s, output: 2077.53 toks/s]
Processed prompts:  87%|████████▋ | 89/102 [00:06<00:00, 60.24it/s, est. speed input: 17307.13 toks/s, output: 2328.87 toks/s]
Processed prompts:  95%|█████████▌| 97/102 [00:06<00:00, 44.75it/s, est. speed input: 18549.47 toks/s, output: 2495.63 toks/s]
Processed prompts: 100%|██████████| 102/102 [00:07<00:00, 13.20it/s, est. speed input: 16177.58 toks/s, output: 2265.50 toks/s]
[rank0]:[W710 15:54:11.924939203 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/asym/ae_rag_lp_sc_test_75_4_output.json
Cleaned temporary files.
Keeping ['ae', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-10 15:54:14.805836_lp_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-10 15:54:14.805836_lp_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-10 15:54:14.805836_lp_processed_test.json', '--batch_size', '4']

  0%|          | 0/136 [00:00<?, ?it/s]
  1%|          | 1/136 [00:00<00:28,  4.77it/s]
  2%|▏         | 3/136 [00:00<00:12, 10.42it/s]
  4%|▎         | 5/136 [00:01<00:44,  2.92it/s]
  5%|▌         | 7/136 [00:01<00:28,  4.46it/s]
  7%|▋         | 9/136 [00:01<00:20,  6.30it/s]
  8%|▊         | 11/136 [00:02<00:19,  6.38it/s]
 10%|▉         | 13/136 [00:02<00:16,  7.49it/s]
 11%|█         | 15/136 [00:02<00:16,  7.15it/s]
 12%|█▏        | 16/136 [00:02<00:22,  5.23it/s]
 12%|█▎        | 17/136 [00:03<00:27,  4.33it/s]
 13%|█▎        | 18/136 [00:03<00:24,  4.89it/s]
 14%|█▍        | 19/136 [00:03<00:25,  4.59it/s]
 15%|█▍        | 20/136 [00:03<00:28,  4.12it/s]
 16%|█▌        | 22/136 [00:04<00:18,  6.19it/s]
 18%|█▊        | 25/136 [00:04<00:24,  4.57it/s]
 19%|█▉        | 26/136 [00:05<00:21,  5.04it/s]
 21%|██▏       | 29/136 [00:05<00:16,  6.48it/s]
 22%|██▏       | 30/136 [00:05<00:15,  6.78it/s]
 23%|██▎       | 31/136 [00:05<00:14,  7.14it/s]
 24%|██▎       | 32/136 [00:05<00:17,  6.03it/s]
 26%|██▌       | 35/136 [00:05<00:11,  9.06it/s]
 27%|██▋       | 37/136 [00:06<00:09, 10.79it/s]
 29%|██▊       | 39/136 [00:06<00:08, 11.90it/s]
 30%|███       | 41/136 [00:06<00:07, 12.52it/s]
 32%|███▏      | 44/136 [00:06<00:06, 14.95it/s]
 34%|███▍      | 46/136 [00:07<00:14,  6.08it/s]
 35%|███▌      | 48/136 [00:07<00:15,  5.84it/s]
 36%|███▌      | 49/136 [00:08<00:17,  5.08it/s]
 40%|███▉      | 54/136 [00:08<00:10,  7.61it/s]
 42%|████▏     | 57/136 [00:08<00:08,  9.25it/s]
 43%|████▎     | 59/136 [00:08<00:07, 10.25it/s]
 45%|████▍     | 61/136 [00:08<00:07, 10.04it/s]
 47%|████▋     | 64/136 [00:09<00:06, 10.43it/s]
 49%|████▊     | 66/136 [00:09<00:06, 10.61it/s]
 51%|█████     | 69/136 [00:09<00:08,  8.33it/s]
 51%|█████▏    | 70/136 [00:10<00:09,  7.11it/s]
 52%|█████▏    | 71/136 [00:10<00:10,  6.20it/s]
 53%|█████▎    | 72/136 [00:10<00:11,  5.52it/s]
 54%|█████▎    | 73/136 [00:10<00:11,  5.49it/s]
 55%|█████▌    | 75/136 [00:11<00:10,  6.06it/s]
 57%|█████▋    | 77/136 [00:11<00:14,  4.13it/s]
 57%|█████▋    | 78/136 [00:12<00:19,  2.98it/s]
 58%|█████▊    | 79/136 [00:13<00:24,  2.34it/s]
 59%|█████▉    | 80/136 [00:14<00:28,  1.99it/s]
 60%|█████▉    | 81/136 [00:14<00:30,  1.80it/s]
 60%|██████    | 82/136 [00:15<00:32,  1.66it/s]
 62%|██████▏   | 84/136 [00:15<00:19,  2.65it/s]
 62%|██████▎   | 85/136 [00:16<00:23,  2.16it/s]
 63%|██████▎   | 86/136 [00:17<00:26,  1.89it/s]
 64%|██████▍   | 87/136 [00:17<00:22,  2.18it/s]
 65%|██████▍   | 88/136 [00:17<00:19,  2.47it/s]
 66%|██████▌   | 90/136 [00:17<00:12,  3.77it/s]
 68%|██████▊   | 93/136 [00:18<00:07,  5.77it/s]
 69%|██████▉   | 94/136 [00:18<00:08,  5.13it/s]
 70%|██████▉   | 95/136 [00:18<00:10,  4.09it/s]
 71%|███████   | 96/136 [00:19<00:09,  4.02it/s]
 73%|███████▎  | 99/136 [00:19<00:05,  6.81it/s]
 74%|███████▍  | 101/136 [00:19<00:04,  8.49it/s]
 76%|███████▌  | 103/136 [00:19<00:04,  8.01it/s]
 77%|███████▋  | 105/136 [00:19<00:03,  9.20it/s]
 79%|███████▉  | 108/136 [00:20<00:05,  5.54it/s]
 80%|████████  | 109/136 [00:21<00:09,  2.71it/s]
 82%|████████▏ | 111/136 [00:22<00:07,  3.33it/s]
 83%|████████▎ | 113/136 [00:22<00:06,  3.46it/s]
 84%|████████▍ | 114/136 [00:22<00:05,  3.82it/s]
 85%|████████▍ | 115/136 [00:23<00:05,  4.13it/s]
 87%|████████▋ | 118/136 [00:23<00:02,  6.12it/s]
 88%|████████▊ | 120/136 [00:23<00:02,  6.63it/s]
 90%|████████▉ | 122/136 [00:23<00:01,  7.55it/s]
 91%|█████████ | 124/136 [00:24<00:01,  6.15it/s]
 93%|█████████▎| 126/136 [00:24<00:01,  5.96it/s]
 93%|█████████▎| 127/136 [00:25<00:02,  3.76it/s]
 96%|█████████▋| 131/136 [00:26<00:01,  3.17it/s]
 98%|█████████▊| 133/136 [00:26<00:00,  4.01it/s]
 99%|█████████▉| 135/136 [00:27<00:00,  4.46it/s]
100%|██████████| 136/136 [00:27<00:00,  3.89it/s]
100%|██████████| 136/136 [00:27<00:00,  4.93it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-10 15:54:14.805836_lp_processed_test.json', '--output_addr', 'data/out/rag/asym/lp_rag_ae_sc_test_75_4_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '4', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 136 examples [00:00, 1213.86 examples/s]
INFO 07-10 15:55:06 config.py:510] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 07-10 15:55:06 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-10 15:55:08 selector.py:120] Using Flash Attention backend.
INFO 07-10 15:55:09 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-10 15:55:09 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.68it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.69it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.75it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.72it/s]

INFO 07-10 15:55:12 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-10 15:55:13 worker.py:241] Memory profiling takes 1.05 seconds
INFO 07-10 15:55:13 worker.py:241] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 07-10 15:55:13 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.44GiB.
INFO 07-10 15:55:13 gpu_executor.py:76] # GPU blocks: 61364, # CPU blocks: 4681
INFO 07-10 15:55:13 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 29.96x
INFO 07-10 15:55:16 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:11,  3.01it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:09,  3.55it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:00<00:08,  3.64it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:08,  3.79it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:07,  3.80it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:07,  3.85it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:01<00:07,  3.88it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:06,  3.89it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:02<00:06,  3.95it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:02<00:06,  3.90it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:02<00:06,  3.94it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:05,  3.92it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:03<00:05,  3.95it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:03<00:05,  3.92it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:03<00:05,  3.93it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:04<00:04,  3.92it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:04<00:04,  3.86it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:04<00:04,  3.92it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:04<00:04,  3.89it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:05<00:03,  3.95it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:05<00:03,  3.92it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:05<00:03,  3.95it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:05<00:03,  3.92it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:06<00:02,  3.94it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:06<00:02,  3.99it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:06<00:02,  3.94it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:06<00:02,  3.97it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:07<00:01,  3.76it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:07<00:01,  3.74it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:07<00:01,  3.74it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:08<00:01,  3.73it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:08<00:00,  3.84it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:08<00:00,  3.85it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:08<00:00,  3.90it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.87it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.86it/s]
INFO 07-10 15:55:25 model_runner.py:1535] Graph capturing finished in 9 secs, took 0.30 GiB
INFO 07-10 15:55:25 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 12.83 seconds
None

Processed prompts:   0%|          | 0/136 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/136 [00:05<11:41,  5.20s/it, est. speed input: 172.70 toks/s, output: 18.85 toks/s]
Processed prompts:   1%|▏         | 2/136 [00:05<04:58,  2.23s/it, est. speed input: 454.00 toks/s, output: 37.97 toks/s]
Processed prompts:   4%|▎         | 5/136 [00:05<01:28,  1.48it/s, est. speed input: 988.60 toks/s, output: 96.20 toks/s]
Processed prompts:   5%|▌         | 7/136 [00:05<00:54,  2.37it/s, est. speed input: 1307.38 toks/s, output: 136.03 toks/s]
Processed prompts:  11%|█         | 15/136 [00:05<00:17,  7.10it/s, est. speed input: 2398.86 toks/s, output: 301.52 toks/s]
Processed prompts:  13%|█▎        | 18/136 [00:06<00:15,  7.39it/s, est. speed input: 2686.98 toks/s, output: 346.94 toks/s]
Processed prompts:  18%|█▊        | 24/136 [00:06<00:09, 11.88it/s, est. speed input: 3499.96 toks/s, output: 470.36 toks/s]
Processed prompts:  29%|██▊       | 39/136 [00:06<00:03, 26.41it/s, est. speed input: 5505.23 toks/s, output: 791.99 toks/s]
Processed prompts:  38%|███▊      | 52/136 [00:06<00:02, 39.36it/s, est. speed input: 7231.01 toks/s, output: 1072.67 toks/s]
Processed prompts:  49%|████▊     | 66/136 [00:06<00:01, 54.38it/s, est. speed input: 9297.97 toks/s, output: 1380.95 toks/s]
Processed prompts:  61%|██████    | 83/136 [00:06<00:00, 72.50it/s, est. speed input: 12073.96 toks/s, output: 1764.12 toks/s]
Processed prompts:  69%|██████▉   | 94/136 [00:06<00:00, 76.87it/s, est. speed input: 13634.03 toks/s, output: 2008.75 toks/s]
Processed prompts:  77%|███████▋  | 105/136 [00:06<00:00, 82.15it/s, est. speed input: 15177.47 toks/s, output: 2265.07 toks/s]
Processed prompts:  85%|████████▌ | 116/136 [00:07<00:00, 88.16it/s, est. speed input: 16636.04 toks/s, output: 2534.20 toks/s]
Processed prompts:  93%|█████████▎| 127/136 [00:07<00:00, 68.77it/s, est. speed input: 17661.38 toks/s, output: 2760.62 toks/s]
Processed prompts: 100%|██████████| 136/136 [00:08<00:00, 20.58it/s, est. speed input: 15972.73 toks/s, output: 2640.75 toks/s]
Processed prompts: 100%|██████████| 136/136 [00:08<00:00, 15.73it/s, est. speed input: 15972.73 toks/s, output: 2640.75 toks/s]
[rank0]:[W710 15:55:35.879404015 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/asym/lp_rag_ae_sc_test_75_4_output.json
Cleaned temporary files.
Keeping ['ae', 'lp'] in profile
Temp data file saved at data/processed/TEMP_2025-07-10 15:55:38.003763_sc_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-10 15:55:38.003763_sc_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-10 15:55:38.003763_sc_processed_test.json', '--batch_size', '4']

  0%|          | 0/106 [00:00<?, ?it/s]
  1%|          | 1/106 [00:00<00:27,  3.79it/s]
  3%|▎         | 3/106 [00:00<00:11,  9.08it/s]
  5%|▍         | 5/106 [00:00<00:12,  7.79it/s]
  7%|▋         | 7/106 [00:00<00:10,  9.34it/s]
  8%|▊         | 9/106 [00:01<00:15,  6.42it/s]
 12%|█▏        | 13/106 [00:01<00:08, 11.13it/s]
 15%|█▌        | 16/106 [00:01<00:07, 11.31it/s]
 19%|█▉        | 20/106 [00:02<00:08, 10.22it/s]
 21%|██        | 22/106 [00:02<00:10,  8.36it/s]
 23%|██▎       | 24/106 [00:02<00:11,  7.30it/s]
 24%|██▎       | 25/106 [00:02<00:10,  7.51it/s]
 25%|██▌       | 27/106 [00:03<00:10,  7.29it/s]
 28%|██▊       | 30/106 [00:03<00:07, 10.02it/s]
 31%|███       | 33/106 [00:03<00:05, 12.56it/s]
 33%|███▎      | 35/106 [00:03<00:05, 12.22it/s]
 35%|███▍      | 37/106 [00:05<00:20,  3.44it/s]
 38%|███▊      | 40/106 [00:05<00:13,  4.88it/s]
 40%|███▉      | 42/106 [00:05<00:12,  5.03it/s]
 42%|████▏     | 44/106 [00:06<00:09,  6.21it/s]
 43%|████▎     | 46/106 [00:06<00:10,  5.86it/s]
 45%|████▌     | 48/106 [00:06<00:08,  6.47it/s]
 46%|████▌     | 49/106 [00:07<00:10,  5.29it/s]
 47%|████▋     | 50/106 [00:07<00:09,  5.78it/s]
 50%|█████     | 53/106 [00:07<00:08,  6.19it/s]
 51%|█████     | 54/106 [00:07<00:08,  6.34it/s]
 52%|█████▏    | 55/106 [00:08<00:10,  5.06it/s]
 53%|█████▎    | 56/106 [00:08<00:09,  5.46it/s]
 55%|█████▍    | 58/106 [00:08<00:06,  7.16it/s]
 56%|█████▌    | 59/106 [00:08<00:08,  5.36it/s]
 58%|█████▊    | 61/106 [00:08<00:06,  6.57it/s]
 60%|██████    | 64/106 [00:09<00:04, 10.07it/s]
 62%|██████▏   | 66/106 [00:09<00:03, 11.09it/s]
 65%|██████▌   | 69/106 [00:09<00:02, 13.62it/s]
 67%|██████▋   | 71/106 [00:09<00:04,  7.87it/s]
 69%|██████▉   | 73/106 [00:10<00:03,  8.70it/s]
 71%|███████   | 75/106 [00:10<00:04,  6.27it/s]
 72%|███████▏  | 76/106 [00:10<00:04,  6.59it/s]
 73%|███████▎  | 77/106 [00:10<00:04,  6.67it/s]
 75%|███████▍  | 79/106 [00:11<00:03,  7.60it/s]
 75%|███████▌  | 80/106 [00:11<00:05,  4.93it/s]
 77%|███████▋  | 82/106 [00:11<00:05,  4.72it/s]
 78%|███████▊  | 83/106 [00:12<00:05,  3.95it/s]
 80%|████████  | 85/106 [00:12<00:04,  4.90it/s]
 83%|████████▎ | 88/106 [00:12<00:02,  6.91it/s]
 85%|████████▍ | 90/106 [00:13<00:02,  6.91it/s]
 86%|████████▌ | 91/106 [00:13<00:02,  6.67it/s]
 88%|████████▊ | 93/106 [00:13<00:01,  8.47it/s]
 91%|█████████ | 96/106 [00:13<00:00, 11.53it/s]
 92%|█████████▏| 98/106 [00:13<00:00,  9.45it/s]
 94%|█████████▍| 100/106 [00:14<00:01,  5.57it/s]
 95%|█████████▌| 101/106 [00:14<00:00,  5.90it/s]
 97%|█████████▋| 103/106 [00:15<00:00,  5.26it/s]
 99%|█████████▉| 105/106 [00:15<00:00,  5.09it/s]
100%|██████████| 106/106 [00:15<00:00,  4.70it/s]
100%|██████████| 106/106 [00:15<00:00,  6.68it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-10 15:55:38.003763_sc_processed_test.json', '--output_addr', 'data/out/rag/asym/sc_rag_ae_lp_test_75_4_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '4', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 106 examples [00:00, 1987.07 examples/s]
INFO 07-10 15:56:16 config.py:510] This model supports multiple tasks: {'classify', 'reward', 'embed', 'score', 'generate'}. Defaulting to 'generate'.
INFO 07-10 15:56:16 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-10 15:56:18 selector.py:120] Using Flash Attention backend.
INFO 07-10 15:56:19 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-10 15:56:19 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.72it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.70it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.75it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]

INFO 07-10 15:56:22 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-10 15:56:23 worker.py:241] Memory profiling takes 1.03 seconds
INFO 07-10 15:56:23 worker.py:241] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 07-10 15:56:23 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.44GiB.
INFO 07-10 15:56:23 gpu_executor.py:76] # GPU blocks: 61364, # CPU blocks: 4681
INFO 07-10 15:56:23 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 29.96x
INFO 07-10 15:56:26 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:10,  3.37it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:08,  3.70it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:00<00:08,  3.75it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:08,  3.86it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:07,  3.82it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:01<00:07,  3.89it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:01<00:07,  3.82it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:07,  3.81it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:02<00:06,  3.80it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:02<00:06,  3.81it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:02<00:06,  3.86it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:03<00:05,  3.85it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:03<00:05,  3.91it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:03<00:05,  3.80it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:03<00:05,  3.87it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:04<00:04,  3.86it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:04<00:04,  3.91it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:04<00:04,  3.90it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:04<00:04,  3.91it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:05<00:03,  3.96it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:05<00:03,  3.91it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:05<00:03,  3.96it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:05<00:03,  3.89it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:06<00:02,  3.85it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:06<00:02,  3.81it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:06<00:02,  3.82it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:07<00:02,  3.87it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:07<00:01,  3.81it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:07<00:01,  3.75it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:07<00:01,  3.78it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:08<00:01,  3.83it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:08<00:00,  3.88it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:08<00:00,  3.85it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:09<00:00,  2.59it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  2.77it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.66it/s]
INFO 07-10 15:56:35 model_runner.py:1535] Graph capturing finished in 10 secs, took 0.30 GiB
INFO 07-10 15:56:35 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 13.28 seconds
None

Processed prompts:   0%|          | 0/106 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/106 [00:03<06:42,  3.83s/it, est. speed input: 267.66 toks/s, output: 18.00 toks/s]
Processed prompts:   2%|▏         | 2/106 [00:04<03:15,  1.88s/it, est. speed input: 401.59 toks/s, output: 38.48 toks/s]
Processed prompts:   3%|▎         | 3/106 [00:04<01:50,  1.08s/it, est. speed input: 561.54 toks/s, output: 60.95 toks/s]
Processed prompts:   6%|▌         | 6/106 [00:04<00:39,  2.56it/s, est. speed input: 1068.27 toks/s, output: 129.80 toks/s]
Processed prompts:   9%|▉         | 10/106 [00:04<00:18,  5.09it/s, est. speed input: 2408.01 toks/s, output: 223.00 toks/s]
Processed prompts:  13%|█▎        | 14/106 [00:04<00:11,  8.19it/s, est. speed input: 3182.93 toks/s, output: 321.19 toks/s]
Processed prompts:  21%|██        | 22/106 [00:04<00:05, 15.88it/s, est. speed input: 4598.59 toks/s, output: 526.87 toks/s]
Processed prompts:  27%|██▋       | 29/106 [00:05<00:03, 22.96it/s, est. speed input: 5957.72 toks/s, output: 709.05 toks/s]
Processed prompts:  32%|███▏      | 34/106 [00:05<00:02, 26.39it/s, est. speed input: 6570.25 toks/s, output: 834.90 toks/s]
Processed prompts:  42%|████▏     | 44/106 [00:05<00:01, 39.76it/s, est. speed input: 8709.94 toks/s, output: 1112.44 toks/s]
Processed prompts:  53%|█████▎    | 56/106 [00:05<00:00, 55.68it/s, est. speed input: 10757.89 toks/s, output: 1451.64 toks/s]
Processed prompts:  65%|██████▌   | 69/106 [00:05<00:00, 71.28it/s, est. speed input: 12906.87 toks/s, output: 1825.52 toks/s]
Processed prompts:  75%|███████▍  | 79/106 [00:05<00:00, 76.85it/s, est. speed input: 14714.49 toks/s, output: 2109.67 toks/s]
Processed prompts:  84%|████████▍ | 89/106 [00:05<00:00, 77.76it/s, est. speed input: 16571.15 toks/s, output: 2396.45 toks/s]
Processed prompts:  92%|█████████▏| 98/106 [00:05<00:00, 66.47it/s, est. speed input: 17510.45 toks/s, output: 2634.35 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:06<00:00, 22.89it/s, est. speed input: 16420.50 toks/s, output: 2577.16 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:06<00:00, 15.25it/s, est. speed input: 16420.50 toks/s, output: 2577.16 toks/s]
[rank0]:[W710 15:56:43.558249628 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/asym/sc_rag_ae_lp_test_75_4_output.json
Cleaned temporary files.
