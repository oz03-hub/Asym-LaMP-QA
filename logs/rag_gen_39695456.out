Keeping ['ae', 'lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-11 15:12:35.354532_ae_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-11 15:12:35.354532_ae_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-11 15:12:35.354532_ae_processed_test.json', '--batch_size', '4']

  0%|          | 0/102 [00:00<?, ?it/s]
  1%|          | 1/102 [00:02<03:27,  2.05s/it]
  2%|▏         | 2/102 [00:03<02:51,  1.72s/it]
  3%|▎         | 3/102 [00:05<02:40,  1.63s/it]
  4%|▍         | 4/102 [00:06<02:35,  1.59s/it]
  5%|▍         | 5/102 [00:08<02:30,  1.55s/it]
  6%|▌         | 6/102 [00:09<02:26,  1.53s/it]
  8%|▊         | 8/102 [00:09<01:16,  1.24it/s]
  9%|▉         | 9/102 [00:13<02:38,  1.71s/it]
 10%|▉         | 10/102 [00:14<02:00,  1.31s/it]
 11%|█         | 11/102 [00:18<03:13,  2.12s/it]
 12%|█▏        | 12/102 [00:22<04:07,  2.75s/it]
 13%|█▎        | 13/102 [00:22<02:56,  1.99s/it]
 14%|█▎        | 14/102 [00:27<03:53,  2.65s/it]
 15%|█▍        | 15/102 [00:27<02:45,  1.91s/it]
 16%|█▌        | 16/102 [00:28<02:24,  1.69s/it]
 17%|█▋        | 17/102 [00:29<02:09,  1.53s/it]
 18%|█▊        | 18/102 [00:29<01:32,  1.11s/it]
 20%|█▉        | 20/102 [00:29<00:51,  1.60it/s]
 21%|██        | 21/102 [00:29<00:40,  2.01it/s]
 22%|██▏       | 22/102 [00:30<00:35,  2.28it/s]
 23%|██▎       | 23/102 [00:30<00:31,  2.49it/s]
 24%|██▎       | 24/102 [00:30<00:32,  2.44it/s]
 25%|██▍       | 25/102 [00:31<00:32,  2.38it/s]
 25%|██▌       | 26/102 [00:31<00:31,  2.44it/s]
 27%|██▋       | 28/102 [00:32<00:22,  3.24it/s]
 28%|██▊       | 29/102 [00:32<00:20,  3.50it/s]
 29%|██▉       | 30/102 [00:32<00:17,  4.18it/s]
 30%|███       | 31/102 [00:32<00:15,  4.61it/s]
 31%|███▏      | 32/102 [00:32<00:15,  4.62it/s]
 32%|███▏      | 33/102 [00:32<00:14,  4.85it/s]
 33%|███▎      | 34/102 [00:33<00:12,  5.40it/s]
 34%|███▍      | 35/102 [00:33<00:17,  3.88it/s]
 35%|███▌      | 36/102 [00:33<00:16,  4.05it/s]
 36%|███▋      | 37/102 [00:33<00:15,  4.16it/s]
 37%|███▋      | 38/102 [00:34<00:15,  4.19it/s]
 38%|███▊      | 39/102 [00:34<00:18,  3.34it/s]
 39%|███▉      | 40/102 [00:35<00:21,  2.91it/s]
 41%|████      | 42/102 [00:35<00:17,  3.40it/s]
 43%|████▎     | 44/102 [00:35<00:12,  4.59it/s]
 44%|████▍     | 45/102 [00:36<00:15,  3.72it/s]
 45%|████▌     | 46/102 [00:36<00:14,  3.89it/s]
 46%|████▌     | 47/102 [00:36<00:13,  4.14it/s]
 47%|████▋     | 48/102 [00:37<00:20,  2.57it/s]
 48%|████▊     | 49/102 [00:38<00:23,  2.22it/s]
 49%|████▉     | 50/102 [00:38<00:28,  1.81it/s]
 50%|█████     | 51/102 [00:40<00:47,  1.08it/s]
 51%|█████     | 52/102 [00:42<00:52,  1.05s/it]
 53%|█████▎    | 54/102 [00:42<00:30,  1.58it/s]
 54%|█████▍    | 55/102 [00:42<00:29,  1.59it/s]
 55%|█████▍    | 56/102 [00:46<01:07,  1.46s/it]
 56%|█████▌    | 57/102 [00:46<00:49,  1.10s/it]
 57%|█████▋    | 58/102 [00:47<00:42,  1.04it/s]
 59%|█████▉    | 60/102 [00:47<00:24,  1.68it/s]
 60%|█████▉    | 61/102 [00:48<00:26,  1.57it/s]
 61%|██████    | 62/102 [00:48<00:22,  1.75it/s]
 62%|██████▏   | 63/102 [00:50<00:32,  1.22it/s]
 63%|██████▎   | 64/102 [00:50<00:23,  1.59it/s]
 65%|██████▍   | 66/102 [00:51<00:18,  1.95it/s]
 66%|██████▌   | 67/102 [00:51<00:15,  2.28it/s]
 67%|██████▋   | 68/102 [00:51<00:12,  2.70it/s]
 68%|██████▊   | 69/102 [00:51<00:10,  3.10it/s]
 69%|██████▊   | 70/102 [00:52<00:12,  2.52it/s]
 70%|██████▉   | 71/102 [00:53<00:14,  2.16it/s]
 71%|███████   | 72/102 [00:53<00:11,  2.70it/s]
 72%|███████▏  | 73/102 [00:53<00:08,  3.26it/s]
 73%|███████▎  | 74/102 [00:53<00:07,  3.75it/s]
 74%|███████▎  | 75/102 [00:53<00:06,  4.39it/s]
 75%|███████▍  | 76/102 [00:54<00:08,  3.08it/s]
 75%|███████▌  | 77/102 [00:54<00:06,  3.58it/s]
 76%|███████▋  | 78/102 [00:54<00:05,  4.27it/s]
 77%|███████▋  | 79/102 [00:54<00:04,  4.88it/s]
 78%|███████▊  | 80/102 [00:55<00:06,  3.16it/s]
 79%|███████▉  | 81/102 [00:55<00:05,  3.51it/s]
 81%|████████▏ | 83/102 [00:57<00:09,  1.96it/s]
 82%|████████▏ | 84/102 [00:58<00:13,  1.34it/s]
 83%|████████▎ | 85/102 [00:59<00:12,  1.37it/s]
 84%|████████▍ | 86/102 [00:59<00:11,  1.42it/s]
 85%|████████▌ | 87/102 [01:00<00:08,  1.70it/s]
 86%|████████▋ | 88/102 [01:00<00:07,  1.79it/s]
 87%|████████▋ | 89/102 [01:01<00:06,  1.86it/s]
 88%|████████▊ | 90/102 [01:01<00:06,  1.91it/s]
 89%|████████▉ | 91/102 [01:01<00:04,  2.47it/s]
 90%|█████████ | 92/102 [01:01<00:03,  2.63it/s]
 91%|█████████ | 93/102 [01:02<00:03,  2.70it/s]
 92%|█████████▏| 94/102 [01:02<00:03,  2.59it/s]
 94%|█████████▍| 96/102 [01:03<00:02,  2.61it/s]
 95%|█████████▌| 97/102 [01:04<00:02,  1.88it/s]
 96%|█████████▌| 98/102 [01:05<00:02,  1.54it/s]
 97%|█████████▋| 99/102 [01:05<00:01,  1.93it/s]
 98%|█████████▊| 100/102 [01:06<00:01,  1.34it/s]
 99%|█████████▉| 101/102 [01:07<00:00,  1.43it/s]
100%|██████████| 102/102 [01:07<00:00,  1.74it/s]
100%|██████████| 102/102 [01:07<00:00,  1.50it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-11 15:12:35.354532_ae_processed_test.json', '--output_addr', 'data/out/rag/full_profile/ae_rag_full_profile_test_75_6_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '6', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 102 examples [00:00, 682.27 examples/s]
INFO 07-11 15:14:16 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'generate', 'score', 'reward'}. Defaulting to 'generate'.
INFO 07-11 15:14:16 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-11 15:14:17 selector.py:120] Using Flash Attention backend.
INFO 07-11 15:14:19 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-11 15:14:19 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:12<00:36, 12.32s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:24<00:24, 12.18s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:34<00:11, 11.23s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:45<00:00, 11.30s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:45<00:00, 11.48s/it]

INFO 07-11 15:15:05 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-11 15:15:07 worker.py:241] Memory profiling takes 2.19 seconds
INFO 07-11 15:15:07 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-11 15:15:07 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-11 15:15:08 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-11 15:15:08 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-11 15:15:11 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.07it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:15,  2.17it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.15it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.18it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.20it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.16it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.19it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.21it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:11,  2.19it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.21it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:10,  2.19it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.20it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:09,  2.22it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.20it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.19it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.21it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.20it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.23it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.21it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.22it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.24it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.23it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.25it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.27it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.23it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:03,  2.26it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.27it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.25it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.27it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.26it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:13<00:01,  2.25it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.26it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:14<00:00,  2.21it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.25it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.25it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.22it/s]
INFO 07-11 15:15:27 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.22 GiB
INFO 07-11 15:15:27 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 21.88 seconds
None

Processed prompts:   0%|          | 0/102 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/102 [00:11<18:43, 11.12s/it, est. speed input: 110.32 toks/s, output: 6.83 toks/s]
Processed prompts:   2%|▏         | 2/102 [00:12<08:58,  5.38s/it, est. speed input: 232.82 toks/s, output: 15.62 toks/s]
Processed prompts:   5%|▍         | 5/102 [00:12<02:31,  1.57s/it, est. speed input: 450.35 toks/s, output: 44.21 toks/s]
Processed prompts:   7%|▋         | 7/102 [00:12<01:31,  1.04it/s, est. speed input: 634.94 toks/s, output: 63.52 toks/s]
Processed prompts:  12%|█▏        | 12/102 [00:12<00:36,  2.45it/s, est. speed input: 1133.49 toks/s, output: 113.38 toks/s]
Processed prompts:  15%|█▍        | 15/102 [00:13<00:25,  3.48it/s, est. speed input: 1448.97 toks/s, output: 143.44 toks/s]
Processed prompts:  20%|█▉        | 20/102 [00:13<00:13,  5.88it/s, est. speed input: 1977.28 toks/s, output: 195.30 toks/s]
Processed prompts:  25%|██▌       | 26/102 [00:13<00:08,  9.34it/s, est. speed input: 2643.45 toks/s, output: 257.87 toks/s]
Processed prompts:  32%|███▏      | 33/102 [00:13<00:04, 14.18it/s, est. speed input: 3473.09 toks/s, output: 332.61 toks/s]
Processed prompts:  38%|███▊      | 39/102 [00:13<00:03, 18.05it/s, est. speed input: 4299.48 toks/s, output: 396.90 toks/s]
Processed prompts:  43%|████▎     | 44/102 [00:13<00:02, 21.47it/s, est. speed input: 4776.81 toks/s, output: 451.71 toks/s]
Processed prompts:  50%|█████     | 51/102 [00:13<00:01, 27.93it/s, est. speed input: 5461.30 toks/s, output: 531.58 toks/s]
Processed prompts:  56%|█████▌    | 57/102 [00:13<00:01, 32.69it/s, est. speed input: 6013.52 toks/s, output: 600.75 toks/s]
Processed prompts:  67%|██████▋   | 68/102 [00:14<00:00, 41.14it/s, est. speed input: 7209.94 toks/s, output: 729.24 toks/s]
Processed prompts:  73%|███████▎  | 74/102 [00:14<00:00, 36.36it/s, est. speed input: 7703.87 toks/s, output: 796.56 toks/s]
Processed prompts:  77%|███████▋  | 79/102 [00:14<00:00, 38.36it/s, est. speed input: 8305.49 toks/s, output: 858.62 toks/s]
Processed prompts:  83%|████████▎ | 85/102 [00:14<00:00, 40.16it/s, est. speed input: 8972.62 toks/s, output: 934.54 toks/s]
Processed prompts:  88%|████████▊ | 90/102 [00:14<00:00, 36.71it/s, est. speed input: 9480.69 toks/s, output: 996.69 toks/s]
Processed prompts:  93%|█████████▎| 95/102 [00:15<00:00, 25.47it/s, est. speed input: 9823.19 toks/s, output: 1050.04 toks/s]
Processed prompts:  97%|█████████▋| 99/102 [00:16<00:00,  8.36it/s, est. speed input: 9421.44 toks/s, output: 1026.86 toks/s]
Processed prompts: 100%|██████████| 102/102 [00:17<00:00,  8.34it/s, est. speed input: 9412.18 toks/s, output: 1073.08 toks/s]
Processed prompts: 100%|██████████| 102/102 [00:17<00:00,  5.98it/s, est. speed input: 9412.18 toks/s, output: 1073.08 toks/s]
[rank0]:[W711 15:15:47.747388182 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/full_profile/ae_rag_full_profile_test_75_6_output.json
Cleaned temporary files.
Keeping ['ae', 'lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-11 15:15:51.079449_lp_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-11 15:15:51.079449_lp_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-11 15:15:51.079449_lp_processed_test.json', '--batch_size', '4']

  0%|          | 0/136 [00:00<?, ?it/s]
  1%|          | 1/136 [00:00<00:42,  3.18it/s]
  1%|▏         | 2/136 [00:00<00:28,  4.78it/s]
  2%|▏         | 3/136 [00:00<00:26,  5.06it/s]
  3%|▎         | 4/136 [00:02<01:32,  1.43it/s]
  4%|▎         | 5/136 [00:02<01:39,  1.32it/s]
  4%|▍         | 6/136 [00:03<01:13,  1.76it/s]
  6%|▌         | 8/136 [00:03<00:44,  2.86it/s]
  7%|▋         | 10/136 [00:03<00:34,  3.63it/s]
  8%|▊         | 11/136 [00:04<00:42,  2.91it/s]
  9%|▉         | 12/136 [00:04<00:40,  3.08it/s]
 10%|▉         | 13/136 [00:04<00:34,  3.58it/s]
 11%|█         | 15/136 [00:05<00:41,  2.94it/s]
 12%|█▏        | 16/136 [00:06<00:55,  2.16it/s]
 12%|█▎        | 17/136 [00:07<01:17,  1.54it/s]
 13%|█▎        | 18/136 [00:07<01:05,  1.80it/s]
 14%|█▍        | 19/136 [00:08<01:16,  1.53it/s]
 15%|█▍        | 20/136 [00:10<01:31,  1.27it/s]
 15%|█▌        | 21/136 [00:10<01:10,  1.63it/s]
 16%|█▌        | 22/136 [00:10<00:53,  2.11it/s]
 18%|█▊        | 25/136 [00:12<01:02,  1.77it/s]
 19%|█▉        | 26/136 [00:12<00:56,  1.94it/s]
 21%|██        | 28/136 [00:12<00:38,  2.79it/s]
 21%|██▏       | 29/136 [00:13<00:45,  2.37it/s]
 22%|██▏       | 30/136 [00:13<00:43,  2.43it/s]
 23%|██▎       | 31/136 [00:14<00:41,  2.54it/s]
 24%|██▎       | 32/136 [00:15<00:59,  1.75it/s]
 24%|██▍       | 33/136 [00:15<00:46,  2.21it/s]
 25%|██▌       | 34/136 [00:15<00:36,  2.81it/s]
 26%|██▌       | 35/136 [00:15<00:30,  3.29it/s]
 26%|██▋       | 36/136 [00:15<00:26,  3.84it/s]
 28%|██▊       | 38/136 [00:16<00:23,  4.21it/s]
 29%|██▉       | 40/136 [00:16<00:16,  5.65it/s]
 30%|███       | 41/136 [00:16<00:19,  4.94it/s]
 32%|███▏      | 43/136 [00:16<00:15,  5.99it/s]
 32%|███▏      | 44/136 [00:17<00:15,  5.91it/s]
 33%|███▎      | 45/136 [00:18<00:51,  1.78it/s]
 35%|███▍      | 47/136 [00:19<00:33,  2.64it/s]
 35%|███▌      | 48/136 [00:19<00:42,  2.05it/s]
 36%|███▌      | 49/136 [00:20<00:49,  1.74it/s]
 38%|███▊      | 52/136 [00:20<00:25,  3.28it/s]
 40%|███▉      | 54/136 [00:21<00:28,  2.87it/s]
 41%|████      | 56/136 [00:22<00:22,  3.58it/s]
 42%|████▏     | 57/136 [00:22<00:20,  3.77it/s]
 43%|████▎     | 58/136 [00:22<00:19,  4.03it/s]
 43%|████▎     | 59/136 [00:22<00:17,  4.39it/s]
 44%|████▍     | 60/136 [00:22<00:19,  3.81it/s]
 45%|████▍     | 61/136 [00:23<00:18,  4.03it/s]
 46%|████▋     | 63/136 [00:23<00:12,  5.71it/s]
 47%|████▋     | 64/136 [00:24<00:20,  3.49it/s]
 48%|████▊     | 65/136 [00:24<00:18,  3.91it/s]
 49%|████▊     | 66/136 [00:24<00:19,  3.60it/s]
 49%|████▉     | 67/136 [00:24<00:16,  4.18it/s]
 50%|█████     | 68/136 [00:24<00:14,  4.68it/s]
 51%|█████     | 69/136 [00:25<00:28,  2.32it/s]
 51%|█████▏    | 70/136 [00:26<00:32,  2.02it/s]
 52%|█████▏    | 71/136 [00:27<00:35,  1.86it/s]
 53%|█████▎    | 72/136 [00:28<00:43,  1.47it/s]
 54%|█████▎    | 73/136 [00:28<00:38,  1.63it/s]
 55%|█████▌    | 75/136 [00:29<00:35,  1.70it/s]
 57%|█████▋    | 77/136 [00:31<00:39,  1.49it/s]
 57%|█████▋    | 78/136 [00:32<00:49,  1.17it/s]
 58%|█████▊    | 79/136 [00:34<00:57,  1.02s/it]
 59%|█████▉    | 80/136 [00:35<01:03,  1.14s/it]
 60%|█████▉    | 81/136 [00:37<01:07,  1.22s/it]
 60%|██████    | 82/136 [00:38<01:10,  1.30s/it]
 61%|██████    | 83/136 [00:39<00:53,  1.01s/it]
 62%|██████▏   | 84/136 [00:39<00:41,  1.26it/s]
 62%|██████▎   | 85/136 [00:40<00:51,  1.01s/it]
 63%|██████▎   | 86/136 [00:42<00:58,  1.16s/it]
 64%|██████▍   | 87/136 [00:43<00:53,  1.08s/it]
 65%|██████▍   | 88/136 [00:44<00:48,  1.02s/it]
 65%|██████▌   | 89/136 [00:44<00:35,  1.31it/s]
 66%|██████▌   | 90/136 [00:44<00:29,  1.57it/s]
 67%|██████▋   | 91/136 [00:44<00:21,  2.09it/s]
 68%|██████▊   | 93/136 [00:45<00:17,  2.48it/s]
 69%|██████▉   | 94/136 [00:46<00:21,  1.96it/s]
 70%|██████▉   | 95/136 [00:47<00:26,  1.53it/s]
 71%|███████   | 96/136 [00:48<00:30,  1.31it/s]
 71%|███████▏  | 97/136 [00:48<00:22,  1.73it/s]
 72%|███████▏  | 98/136 [00:48<00:17,  2.22it/s]
 73%|███████▎  | 99/136 [00:48<00:13,  2.78it/s]
 74%|███████▎  | 100/136 [00:48<00:10,  3.31it/s]
 74%|███████▍  | 101/136 [00:48<00:08,  4.03it/s]
 75%|███████▌  | 102/136 [00:50<00:17,  1.92it/s]
 76%|███████▋  | 104/136 [00:50<00:11,  2.81it/s]
 77%|███████▋  | 105/136 [00:50<00:09,  3.37it/s]
 79%|███████▊  | 107/136 [00:50<00:06,  4.49it/s]
 79%|███████▉  | 108/136 [00:52<00:16,  1.71it/s]
 80%|████████  | 109/136 [00:56<00:36,  1.36s/it]
 81%|████████  | 110/136 [00:56<00:27,  1.04s/it]
 82%|████████▏ | 111/136 [00:57<00:26,  1.05s/it]
 82%|████████▏ | 112/136 [00:57<00:19,  1.23it/s]
 83%|████████▎ | 113/136 [00:58<00:19,  1.21it/s]
 84%|████████▍ | 114/136 [00:59<00:15,  1.39it/s]
 85%|████████▍ | 115/136 [00:59<00:13,  1.59it/s]
 85%|████████▌ | 116/136 [00:59<00:09,  2.11it/s]
 87%|████████▋ | 118/136 [01:00<00:07,  2.43it/s]
 88%|████████▊ | 119/136 [01:00<00:06,  2.71it/s]
 88%|████████▊ | 120/136 [01:01<00:06,  2.38it/s]
 90%|████████▉ | 122/136 [01:01<00:05,  2.80it/s]
 91%|█████████ | 124/136 [01:02<00:04,  2.48it/s]
 93%|█████████▎| 126/136 [01:03<00:04,  2.34it/s]
 93%|█████████▎| 127/136 [01:05<00:05,  1.55it/s]
 94%|█████████▍| 128/136 [01:05<00:04,  1.83it/s]
 96%|█████████▌| 130/136 [01:05<00:02,  2.81it/s]
 96%|█████████▋| 131/136 [01:09<00:05,  1.10s/it]
 97%|█████████▋| 132/136 [01:09<00:03,  1.16it/s]
 98%|█████████▊| 133/136 [01:09<00:02,  1.35it/s]
 99%|█████████▊| 134/136 [01:09<00:01,  1.70it/s]
 99%|█████████▉| 135/136 [01:10<00:00,  1.39it/s]
100%|██████████| 136/136 [01:11<00:00,  1.30it/s]
100%|██████████| 136/136 [01:11<00:00,  1.90it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-11 15:15:51.079449_lp_processed_test.json', '--output_addr', 'data/out/rag/full_profile/lp_rag_full_profile_test_75_6_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '6', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 136 examples [00:00, 807.85 examples/s]
INFO 07-11 15:17:30 config.py:510] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 07-11 15:17:30 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-11 15:17:32 selector.py:120] Using Flash Attention backend.
INFO 07-11 15:17:33 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-11 15:17:33 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.42it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.33it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.37it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.34it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]

INFO 07-11 15:17:37 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-11 15:17:39 worker.py:241] Memory profiling takes 2.18 seconds
INFO 07-11 15:17:39 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-11 15:17:39 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-11 15:17:39 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-11 15:17:39 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-11 15:17:43 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:17,  2.00it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:15,  2.13it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.16it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.15it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.18it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.16it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.18it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.20it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:12,  2.15it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.18it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:10,  2.20it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.19it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:10,  2.16it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.16it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.18it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.20it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.20it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.23it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.25it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.21it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.23it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:05,  2.24it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.20it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.23it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.26it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:04,  2.24it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.26it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.23it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.24it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.27it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.24it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.26it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:14<00:00,  2.24it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.24it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.26it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.21it/s]
INFO 07-11 15:17:59 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.22 GiB
INFO 07-11 15:17:59 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.15 seconds
None

Processed prompts:   0%|          | 0/136 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/136 [00:14<33:22, 14.83s/it, est. speed input: 59.73 toks/s, output: 7.62 toks/s]
Processed prompts:   1%|▏         | 2/136 [00:15<14:23,  6.44s/it, est. speed input: 107.50 toks/s, output: 15.00 toks/s]
Processed prompts:   2%|▏         | 3/136 [00:15<07:52,  3.56s/it, est. speed input: 219.94 toks/s, output: 22.68 toks/s]
Processed prompts:   4%|▍         | 6/136 [00:15<02:40,  1.24s/it, est. speed input: 442.26 toks/s, output: 46.05 toks/s]
Processed prompts:   7%|▋         | 9/136 [00:15<01:23,  1.52it/s, est. speed input: 666.40 toks/s, output: 69.69 toks/s]
Processed prompts:   9%|▉         | 12/136 [00:15<00:50,  2.47it/s, est. speed input: 977.14 toks/s, output: 93.82 toks/s]
Processed prompts:  13%|█▎        | 18/136 [00:16<00:23,  5.11it/s, est. speed input: 1345.96 toks/s, output: 143.22 toks/s]
Processed prompts:  16%|█▌        | 22/136 [00:16<00:16,  7.11it/s, est. speed input: 1746.75 toks/s, output: 175.79 toks/s]
Processed prompts:  19%|█▉        | 26/136 [00:16<00:11,  9.72it/s, est. speed input: 2057.22 toks/s, output: 209.30 toks/s]
Processed prompts:  22%|██▏       | 30/136 [00:16<00:08, 12.39it/s, est. speed input: 2408.23 toks/s, output: 242.96 toks/s]
Processed prompts:  27%|██▋       | 37/136 [00:16<00:05, 18.23it/s, est. speed input: 3082.38 toks/s, output: 303.27 toks/s]
Processed prompts:  36%|███▌      | 49/136 [00:16<00:02, 31.70it/s, est. speed input: 4020.67 toks/s, output: 411.20 toks/s]
Processed prompts:  40%|████      | 55/136 [00:16<00:02, 35.54it/s, est. speed input: 4469.78 toks/s, output: 464.90 toks/s]
Processed prompts:  45%|████▍     | 61/136 [00:16<00:01, 39.30it/s, est. speed input: 5009.18 toks/s, output: 519.10 toks/s]
Processed prompts:  51%|█████▏    | 70/136 [00:17<00:01, 46.46it/s, est. speed input: 5681.89 toks/s, output: 602.31 toks/s]
Processed prompts:  56%|█████▌    | 76/136 [00:17<00:01, 46.66it/s, est. speed input: 6121.31 toks/s, output: 657.73 toks/s]
Processed prompts:  60%|██████    | 82/136 [00:17<00:01, 45.30it/s, est. speed input: 6568.14 toks/s, output: 713.25 toks/s]
Processed prompts:  69%|██████▉   | 94/136 [00:17<00:00, 60.89it/s, est. speed input: 7436.85 toks/s, output: 833.58 toks/s]
Processed prompts:  76%|███████▌  | 103/136 [00:17<00:00, 67.70it/s, est. speed input: 8247.28 toks/s, output: 924.69 toks/s]
Processed prompts:  82%|████████▏ | 111/136 [00:17<00:00, 69.83it/s, est. speed input: 8842.46 toks/s, output: 1005.88 toks/s]
Processed prompts:  88%|████████▊ | 119/136 [00:17<00:00, 59.83it/s, est. speed input: 9391.63 toks/s, output: 1086.10 toks/s]
Processed prompts:  93%|█████████▎| 126/136 [00:18<00:00, 33.45it/s, est. speed input: 9679.85 toks/s, output: 1140.68 toks/s]
Processed prompts:  97%|█████████▋| 132/136 [00:19<00:00, 14.97it/s, est. speed input: 9656.40 toks/s, output: 1161.22 toks/s]
Processed prompts: 100%|██████████| 136/136 [00:20<00:00, 10.96it/s, est. speed input: 9582.03 toks/s, output: 1185.37 toks/s]
Processed prompts: 100%|██████████| 136/136 [00:20<00:00,  6.76it/s, est. speed input: 9582.03 toks/s, output: 1185.37 toks/s]
{  "personalized_answer": "Absolutely, you can exit the airport for your 7-hour layover in Paris! With such a long layover, it's a great opportunity to explore the city. Paris is full of amazing sights, from the iconic Eiffel Tower to the charming streets of Montmartre. Just make sure to keep your boarding pass and ID handy, and perhaps consider purchasing a一日通行证 (yī rì tōng xìng) to save on transportation costs as you navigate the city. Enjoy your time in Paris and have a fantastic trip!"}Note: The phrase "一日通行证" (yī rì tōng xìng) is a literal translation of "one-day pass" which is commonly used in China for city exploration. Since the user's past questions indicate an interest in exploring and making the most of their time, I've included this suggestion.
Invalid JSON
None

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, est. speed input: 783.25 toks/s, output: 79.41 toks/s]
Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, est. speed input: 783.25 toks/s, output: 79.41 toks/s]
[rank0]:[W711 15:18:22.480866507 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/full_profile/lp_rag_full_profile_test_75_6_output.json
Cleaned temporary files.
Keeping ['ae', 'lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-11 15:18:26.807008_sc_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-11 15:18:26.807008_sc_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-11 15:18:26.807008_sc_processed_test.json', '--batch_size', '4']

  0%|          | 0/106 [00:00<?, ?it/s]
  1%|          | 1/106 [00:00<00:48,  2.18it/s]
  2%|▏         | 2/106 [00:00<00:26,  3.90it/s]
  3%|▎         | 3/106 [00:00<00:24,  4.16it/s]
  4%|▍         | 4/106 [00:01<00:38,  2.62it/s]
  5%|▍         | 5/106 [00:01<00:34,  2.89it/s]
  6%|▌         | 6/106 [00:01<00:28,  3.46it/s]
  7%|▋         | 7/106 [00:02<00:40,  2.42it/s]
  8%|▊         | 8/106 [00:04<01:14,  1.31it/s]
  8%|▊         | 9/106 [00:04<00:56,  1.71it/s]
 10%|█         | 11/106 [00:04<00:35,  2.67it/s]
 13%|█▎        | 14/106 [00:04<00:19,  4.84it/s]
 15%|█▌        | 16/106 [00:05<00:22,  3.98it/s]
 17%|█▋        | 18/106 [00:05<00:17,  5.09it/s]
 18%|█▊        | 19/106 [00:05<00:15,  5.59it/s]
 19%|█▉        | 20/106 [00:06<00:28,  3.07it/s]
 20%|█▉        | 21/106 [00:06<00:23,  3.62it/s]
 21%|██        | 22/106 [00:07<00:35,  2.34it/s]
 22%|██▏       | 23/106 [00:07<00:28,  2.86it/s]
 23%|██▎       | 24/106 [00:08<00:34,  2.40it/s]
 24%|██▎       | 25/106 [00:08<00:29,  2.72it/s]
 25%|██▍       | 26/106 [00:08<00:23,  3.36it/s]
 25%|██▌       | 27/106 [00:09<00:32,  2.47it/s]
 26%|██▋       | 28/106 [00:09<00:25,  3.08it/s]
 27%|██▋       | 29/106 [00:09<00:20,  3.81it/s]
 28%|██▊       | 30/106 [00:09<00:17,  4.41it/s]
 29%|██▉       | 31/106 [00:09<00:14,  5.23it/s]
 30%|███       | 32/106 [00:09<00:12,  6.08it/s]
 31%|███       | 33/106 [00:09<00:11,  6.47it/s]
 32%|███▏      | 34/106 [00:10<00:10,  7.02it/s]
 33%|███▎      | 35/106 [00:10<00:15,  4.55it/s]
 34%|███▍      | 36/106 [00:12<00:45,  1.54it/s]
 35%|███▍      | 37/106 [00:16<01:58,  1.72s/it]
 37%|███▋      | 39/106 [00:16<01:04,  1.03it/s]
 38%|███▊      | 40/106 [00:16<00:51,  1.29it/s]
 39%|███▊      | 41/106 [00:17<00:50,  1.28it/s]
 40%|███▉      | 42/106 [00:17<00:39,  1.60it/s]
 41%|████      | 43/106 [00:17<00:31,  1.99it/s]
 42%|████▏     | 45/106 [00:18<00:19,  3.14it/s]
 43%|████▎     | 46/106 [00:18<00:27,  2.19it/s]
 44%|████▍     | 47/106 [00:19<00:23,  2.51it/s]
 45%|████▌     | 48/106 [00:19<00:21,  2.69it/s]
 46%|████▌     | 49/106 [00:20<00:29,  1.94it/s]
 47%|████▋     | 50/106 [00:20<00:25,  2.23it/s]
 49%|████▉     | 52/106 [00:20<00:15,  3.41it/s]
 50%|█████     | 53/106 [00:21<00:24,  2.18it/s]
 51%|█████     | 54/106 [00:22<00:22,  2.28it/s]
 52%|█████▏    | 55/106 [00:23<00:28,  1.78it/s]
 53%|█████▎    | 56/106 [00:23<00:26,  1.89it/s]
 54%|█████▍    | 57/106 [00:23<00:21,  2.33it/s]
 55%|█████▍    | 58/106 [00:24<00:20,  2.39it/s]
 56%|█████▌    | 59/106 [00:25<00:27,  1.73it/s]
 58%|█████▊    | 61/106 [00:25<00:20,  2.16it/s]
 58%|█████▊    | 62/106 [00:25<00:16,  2.61it/s]
 59%|█████▉    | 63/106 [00:25<00:13,  3.17it/s]
 61%|██████▏   | 65/106 [00:26<00:08,  4.94it/s]
 62%|██████▏   | 66/106 [00:26<00:08,  4.90it/s]
 63%|██████▎   | 67/106 [00:26<00:06,  5.58it/s]
 65%|██████▌   | 69/106 [00:26<00:05,  7.16it/s]
 66%|██████▌   | 70/106 [00:27<00:12,  2.88it/s]
 67%|██████▋   | 71/106 [00:27<00:10,  3.25it/s]
 68%|██████▊   | 72/106 [00:28<00:09,  3.48it/s]
 69%|██████▉   | 73/106 [00:28<00:08,  3.78it/s]
 70%|██████▉   | 74/106 [00:29<00:19,  1.65it/s]
 71%|███████   | 75/106 [00:30<00:16,  1.83it/s]
 72%|███████▏  | 76/106 [00:30<00:15,  1.92it/s]
 73%|███████▎  | 77/106 [00:31<00:16,  1.71it/s]
 74%|███████▎  | 78/106 [00:31<00:12,  2.24it/s]
 75%|███████▍  | 79/106 [00:32<00:13,  2.00it/s]
 75%|███████▌  | 80/106 [00:33<00:16,  1.54it/s]
 76%|███████▋  | 81/106 [00:33<00:12,  1.98it/s]
 77%|███████▋  | 82/106 [00:34<00:16,  1.48it/s]
 78%|███████▊  | 83/106 [00:35<00:18,  1.26it/s]
 79%|███████▉  | 84/106 [00:35<00:13,  1.62it/s]
 80%|████████  | 85/106 [00:35<00:11,  1.84it/s]
 81%|████████  | 86/106 [00:36<00:08,  2.41it/s]
 82%|████████▏ | 87/106 [00:36<00:06,  3.08it/s]
 83%|████████▎ | 88/106 [00:36<00:05,  3.01it/s]
 84%|████████▍ | 89/106 [00:36<00:04,  3.74it/s]
 85%|████████▍ | 90/106 [00:37<00:06,  2.52it/s]
 86%|████████▌ | 91/106 [00:37<00:06,  2.22it/s]
 88%|████████▊ | 93/106 [00:38<00:04,  3.22it/s]
 89%|████████▊ | 94/106 [00:38<00:03,  3.50it/s]
 91%|█████████ | 96/106 [00:38<00:02,  4.52it/s]
 92%|█████████▏| 97/106 [00:39<00:03,  2.94it/s]
 92%|█████████▏| 98/106 [00:40<00:03,  2.35it/s]
 93%|█████████▎| 99/106 [00:41<00:04,  1.52it/s]
 94%|█████████▍| 100/106 [00:42<00:05,  1.20it/s]
 95%|█████████▌| 101/106 [00:42<00:03,  1.49it/s]
 96%|█████████▌| 102/106 [00:43<00:02,  1.85it/s]
 97%|█████████▋| 103/106 [00:44<00:02,  1.30it/s]
 99%|█████████▉| 105/106 [00:46<00:00,  1.27it/s]
100%|██████████| 106/106 [00:46<00:00,  1.31it/s]
100%|██████████| 106/106 [00:46<00:00,  2.26it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-11 15:18:26.807008_sc_processed_test.json', '--output_addr', 'data/out/rag/full_profile/sc_rag_full_profile_test_75_6_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '6', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 106 examples [00:00, 950.80 examples/s]
INFO 07-11 15:19:42 config.py:510] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 07-11 15:19:42 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-11 15:19:44 selector.py:120] Using Flash Attention backend.
INFO 07-11 15:19:45 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-11 15:19:45 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.43it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.35it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.39it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.36it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]

INFO 07-11 15:19:49 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-11 15:19:51 worker.py:241] Memory profiling takes 2.22 seconds
INFO 07-11 15:19:51 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-11 15:19:51 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-11 15:19:51 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-11 15:19:51 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-11 15:19:55 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.01it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:15,  2.09it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.15it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.14it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.16it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.18it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.16it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.18it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:12,  2.15it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.16it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:10,  2.19it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.16it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:10,  2.19it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.20it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.18it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.20it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.19it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.21it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.23it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.21it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.23it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:05,  2.25it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.23it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.22it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.21it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:04,  2.22it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.25it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.23it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.25it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.27it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.23it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.25it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:15<00:00,  2.15it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.18it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.21it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.20it/s]
INFO 07-11 15:20:11 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.22 GiB
INFO 07-11 15:20:11 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.31 seconds
None

Processed prompts:   0%|          | 0/106 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/106 [00:11<19:43, 11.27s/it, est. speed input: 111.53 toks/s, output: 6.92 toks/s]
Processed prompts:   2%|▏         | 2/106 [00:12<08:57,  5.17s/it, est. speed input: 200.36 toks/s, output: 15.13 toks/s]
Processed prompts:   4%|▍         | 4/106 [00:12<03:24,  2.01s/it, est. speed input: 417.69 toks/s, output: 32.61 toks/s]
Processed prompts:   6%|▌         | 6/106 [00:12<01:48,  1.09s/it, est. speed input: 708.19 toks/s, output: 50.86 toks/s]
Processed prompts:   8%|▊         | 8/106 [00:12<01:06,  1.48it/s, est. speed input: 868.40 toks/s, output: 69.38 toks/s]
Processed prompts:   9%|▉         | 10/106 [00:12<00:43,  2.19it/s, est. speed input: 1105.17 toks/s, output: 88.20 toks/s]
Processed prompts:  11%|█▏        | 12/106 [00:13<00:32,  2.93it/s, est. speed input: 1433.52 toks/s, output: 106.69 toks/s]
Processed prompts:  15%|█▌        | 16/106 [00:13<00:17,  5.24it/s, est. speed input: 1796.42 toks/s, output: 147.03 toks/s]
Processed prompts:  19%|█▉        | 20/106 [00:13<00:10,  8.06it/s, est. speed input: 2104.63 toks/s, output: 188.32 toks/s]
Processed prompts:  22%|██▏       | 23/106 [00:13<00:08, 10.23it/s, est. speed input: 2484.61 toks/s, output: 219.21 toks/s]
Processed prompts:  30%|███       | 32/106 [00:13<00:03, 19.55it/s, est. speed input: 3371.98 toks/s, output: 316.33 toks/s]
Processed prompts:  36%|███▌      | 38/106 [00:13<00:02, 25.25it/s, est. speed input: 3826.65 toks/s, output: 381.83 toks/s]
Processed prompts:  42%|████▏     | 45/106 [00:13<00:01, 32.94it/s, est. speed input: 4603.65 toks/s, output: 459.65 toks/s]
Processed prompts:  47%|████▋     | 50/106 [00:13<00:01, 34.78it/s, est. speed input: 5145.08 toks/s, output: 514.57 toks/s]
Processed prompts:  52%|█████▏    | 55/106 [00:14<00:01, 33.45it/s, est. speed input: 5682.08 toks/s, output: 568.83 toks/s]
Processed prompts:  61%|██████▏   | 65/106 [00:14<00:00, 46.45it/s, est. speed input: 6695.61 toks/s, output: 687.98 toks/s]
Processed prompts:  67%|██████▋   | 71/106 [00:14<00:00, 49.24it/s, est. speed input: 7324.58 toks/s, output: 758.83 toks/s]
Processed prompts:  73%|███████▎  | 77/106 [00:14<00:00, 48.08it/s, est. speed input: 8015.01 toks/s, output: 829.75 toks/s]
Processed prompts:  82%|████████▏ | 87/106 [00:14<00:00, 53.92it/s, est. speed input: 8984.70 toks/s, output: 954.23 toks/s]
Processed prompts:  88%|████████▊ | 93/106 [00:14<00:00, 46.98it/s, est. speed input: 9469.49 toks/s, output: 1027.17 toks/s]
Processed prompts:  93%|█████████▎| 99/106 [00:15<00:00, 28.23it/s, est. speed input: 9848.07 toks/s, output: 1084.65 toks/s]
Processed prompts:  97%|█████████▋| 103/106 [00:15<00:00, 17.96it/s, est. speed input: 9831.59 toks/s, output: 1115.20 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:17<00:00,  5.74it/s, est. speed input: 9057.67 toks/s, output: 1042.42 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:17<00:00,  5.92it/s, est. speed input: 9057.67 toks/s, output: 1042.42 toks/s]
[rank0]:[W711 15:20:31.946470722 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/full_profile/sc_rag_full_profile_test_75_6_output.json
Cleaned temporary files.
Keeping ['lp', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-11 15:20:35.240544_ae_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-11 15:20:35.240544_ae_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-11 15:20:35.240544_ae_processed_test.json', '--batch_size', '4']

  0%|          | 0/102 [00:00<?, ?it/s]
  1%|          | 1/102 [00:01<01:55,  1.14s/it]
  2%|▏         | 2/102 [00:02<01:40,  1.00s/it]
  3%|▎         | 3/102 [00:02<01:33,  1.06it/s]
  4%|▍         | 4/102 [00:03<01:31,  1.08it/s]
  5%|▍         | 5/102 [00:04<01:27,  1.10it/s]
  6%|▌         | 6/102 [00:05<01:25,  1.13it/s]
  9%|▉         | 9/102 [00:08<01:23,  1.11it/s]
 11%|█         | 11/102 [00:10<01:36,  1.06s/it]
 12%|█▏        | 12/102 [00:13<02:04,  1.38s/it]
 14%|█▎        | 14/102 [00:16<02:00,  1.37s/it]
 15%|█▍        | 15/102 [00:16<01:36,  1.11s/it]
 16%|█▌        | 16/102 [00:17<01:27,  1.02s/it]
 17%|█▋        | 17/102 [00:17<01:19,  1.07it/s]
 19%|█▊        | 19/102 [00:17<00:48,  1.73it/s]
 21%|██        | 21/102 [00:18<00:31,  2.56it/s]
 22%|██▏       | 22/102 [00:18<00:28,  2.82it/s]
 23%|██▎       | 23/102 [00:18<00:25,  3.12it/s]
 24%|██▎       | 24/102 [00:18<00:24,  3.19it/s]
 25%|██▍       | 25/102 [00:19<00:23,  3.28it/s]
 25%|██▌       | 26/102 [00:19<00:20,  3.62it/s]
 27%|██▋       | 28/102 [00:19<00:15,  4.76it/s]
 28%|██▊       | 29/102 [00:19<00:13,  5.43it/s]
 30%|███       | 31/102 [00:19<00:09,  7.47it/s]
 32%|███▏      | 33/102 [00:19<00:08,  8.33it/s]
 34%|███▍      | 35/102 [00:20<00:09,  7.25it/s]
 35%|███▌      | 36/102 [00:20<00:08,  7.62it/s]
 37%|███▋      | 38/102 [00:20<00:08,  7.29it/s]
 38%|███▊      | 39/102 [00:20<00:10,  6.05it/s]
 39%|███▉      | 40/102 [00:21<00:11,  5.35it/s]
 41%|████      | 42/102 [00:21<00:09,  6.09it/s]
 43%|████▎     | 44/102 [00:21<00:07,  7.59it/s]
 44%|████▍     | 45/102 [00:21<00:09,  6.05it/s]
 45%|████▌     | 46/102 [00:22<00:08,  6.23it/s]
 46%|████▌     | 47/102 [00:22<00:08,  6.47it/s]
 47%|████▋     | 48/102 [00:22<00:14,  3.77it/s]
 48%|████▊     | 49/102 [00:23<00:14,  3.64it/s]
 49%|████▉     | 50/102 [00:23<00:18,  2.79it/s]
 50%|█████     | 51/102 [00:25<00:34,  1.47it/s]
 51%|█████     | 52/102 [00:25<00:35,  1.39it/s]
 53%|█████▎    | 54/102 [00:26<00:21,  2.28it/s]
 54%|█████▍    | 55/102 [00:26<00:18,  2.50it/s]
 55%|█████▍    | 56/102 [00:29<00:51,  1.12s/it]
 56%|█████▌    | 57/102 [00:29<00:38,  1.17it/s]
 57%|█████▋    | 58/102 [00:30<00:30,  1.45it/s]
 59%|█████▉    | 60/102 [00:30<00:17,  2.38it/s]
 60%|█████▉    | 61/102 [00:30<00:18,  2.16it/s]
 61%|██████    | 62/102 [00:31<00:17,  2.32it/s]
 62%|██████▏   | 63/102 [00:32<00:21,  1.78it/s]
 64%|██████▎   | 65/102 [00:32<00:12,  2.91it/s]
 65%|██████▍   | 66/102 [00:32<00:13,  2.66it/s]
 67%|██████▋   | 68/102 [00:32<00:08,  3.92it/s]
 69%|██████▊   | 70/102 [00:33<00:07,  4.21it/s]
 70%|██████▉   | 71/102 [00:33<00:08,  3.52it/s]
 72%|███████▏  | 73/102 [00:33<00:05,  4.91it/s]
 74%|███████▎  | 75/102 [00:33<00:04,  6.21it/s]
 75%|███████▍  | 76/102 [00:34<00:05,  4.82it/s]
 75%|███████▌  | 77/102 [00:34<00:04,  5.14it/s]
 77%|███████▋  | 79/102 [00:34<00:03,  6.69it/s]
 78%|███████▊  | 80/102 [00:34<00:03,  5.56it/s]
 79%|███████▉  | 81/102 [00:35<00:03,  5.71it/s]
 81%|████████▏ | 83/102 [00:35<00:05,  3.67it/s]
 82%|████████▏ | 84/102 [00:36<00:07,  2.54it/s]
 83%|████████▎ | 85/102 [00:37<00:07,  2.32it/s]
 84%|████████▍ | 86/102 [00:37<00:06,  2.43it/s]
 85%|████████▌ | 87/102 [00:37<00:05,  2.88it/s]
 86%|████████▋ | 88/102 [00:38<00:04,  3.28it/s]
 87%|████████▋ | 89/102 [00:38<00:03,  3.63it/s]
 88%|████████▊ | 90/102 [00:38<00:03,  3.95it/s]
 90%|█████████ | 92/102 [00:38<00:01,  5.45it/s]
 91%|█████████ | 93/102 [00:38<00:01,  5.24it/s]
 92%|█████████▏| 94/102 [00:39<00:01,  5.06it/s]
 94%|█████████▍| 96/102 [00:39<00:01,  5.36it/s]
 95%|█████████▌| 97/102 [00:40<00:01,  3.07it/s]
 96%|█████████▌| 98/102 [00:40<00:01,  2.30it/s]
 98%|█████████▊| 100/102 [00:41<00:00,  2.27it/s]
 99%|█████████▉| 101/102 [00:42<00:00,  2.36it/s]
100%|██████████| 102/102 [00:42<00:00,  2.60it/s]
100%|██████████| 102/102 [00:42<00:00,  2.40it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-11 15:20:35.240544_ae_processed_test.json', '--output_addr', 'data/out/rag/asym/ae_rag_lp_sc_test_75_6_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '6', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 102 examples [00:00, 965.19 examples/s]
INFO 07-11 15:21:45 config.py:510] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 07-11 15:21:45 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-11 15:21:46 selector.py:120] Using Flash Attention backend.
INFO 07-11 15:21:48 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-11 15:21:48 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.45it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.39it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.43it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]

INFO 07-11 15:21:51 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-11 15:21:53 worker.py:241] Memory profiling takes 2.21 seconds
INFO 07-11 15:21:53 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-11 15:21:53 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-11 15:21:53 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-11 15:21:53 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-11 15:21:57 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.10it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:15,  2.08it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.14it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.15it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.16it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.18it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.17it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.19it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:11,  2.20it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.18it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:10,  2.20it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.19it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:10,  2.19it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.21it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.19it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.21it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.24it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.21it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.24it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.25it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.24it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.26it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.24it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.22it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.25it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:04,  2.22it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.25it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.27it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.26it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.28it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:13<00:01,  2.26it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.26it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:14<00:00,  2.25it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.18it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.22it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.21it/s]
INFO 07-11 15:22:13 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.22 GiB
INFO 07-11 15:22:13 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.25 seconds
None

Processed prompts:   0%|          | 0/102 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/102 [00:12<20:35, 12.23s/it, est. speed input: 84.93 toks/s, output: 7.28 toks/s]
Processed prompts:   2%|▏         | 2/102 [00:12<08:33,  5.13s/it, est. speed input: 228.14 toks/s, output: 14.77 toks/s]
Processed prompts:   4%|▍         | 4/102 [00:12<03:16,  2.00s/it, est. speed input: 464.71 toks/s, output: 30.21 toks/s]
Processed prompts:   5%|▍         | 5/102 [00:12<02:18,  1.42s/it, est. speed input: 566.77 toks/s, output: 38.27 toks/s]
Processed prompts:   6%|▌         | 6/102 [00:12<01:38,  1.03s/it, est. speed input: 772.37 toks/s, output: 46.48 toks/s]
Processed prompts:   8%|▊         | 8/102 [00:13<00:56,  1.66it/s, est. speed input: 983.46 toks/s, output: 63.07 toks/s]
Processed prompts:   9%|▉         | 9/102 [00:13<00:45,  2.04it/s, est. speed input: 1151.96 toks/s, output: 71.65 toks/s]
Processed prompts:  11%|█         | 11/102 [00:13<00:28,  3.14it/s, est. speed input: 1307.84 toks/s, output: 89.69 toks/s]
Processed prompts:  15%|█▍        | 15/102 [00:13<00:14,  6.04it/s, est. speed input: 1661.94 toks/s, output: 127.45 toks/s]
Processed prompts:  22%|██▏       | 22/102 [00:13<00:06, 12.54it/s, est. speed input: 2408.78 toks/s, output: 195.70 toks/s]
Processed prompts:  30%|███       | 31/102 [00:13<00:03, 22.10it/s, est. speed input: 3343.59 toks/s, output: 284.94 toks/s]
Processed prompts:  37%|███▋      | 38/102 [00:13<00:02, 27.89it/s, est. speed input: 4160.81 toks/s, output: 354.20 toks/s]
Processed prompts:  42%|████▏     | 43/102 [00:14<00:01, 30.27it/s, est. speed input: 4888.48 toks/s, output: 404.26 toks/s]
Processed prompts:  48%|████▊     | 49/102 [00:14<00:01, 34.63it/s, est. speed input: 5546.54 toks/s, output: 465.68 toks/s]
Processed prompts:  53%|█████▎    | 54/102 [00:14<00:01, 36.62it/s, est. speed input: 6084.32 toks/s, output: 517.64 toks/s]
Processed prompts:  59%|█████▉    | 60/102 [00:14<00:01, 40.97it/s, est. speed input: 6911.94 toks/s, output: 582.30 toks/s]
Processed prompts:  67%|██████▋   | 68/102 [00:14<00:00, 45.78it/s, est. speed input: 7724.53 toks/s, output: 669.36 toks/s]
Processed prompts:  73%|███████▎  | 74/102 [00:14<00:00, 45.85it/s, est. speed input: 8264.22 toks/s, output: 736.04 toks/s]
Processed prompts:  78%|███████▊  | 80/102 [00:14<00:00, 48.75it/s, est. speed input: 8831.29 toks/s, output: 805.26 toks/s]
Processed prompts:  84%|████████▍ | 86/102 [00:15<00:00, 42.81it/s, est. speed input: 9423.70 toks/s, output: 872.43 toks/s]
Processed prompts:  89%|████████▉ | 91/102 [00:15<00:00, 43.30it/s, est. speed input: 9942.36 toks/s, output: 932.92 toks/s]
Processed prompts:  94%|█████████▍| 96/102 [00:15<00:00, 43.33it/s, est. speed input: 10366.82 toks/s, output: 994.66 toks/s]
Processed prompts:  99%|█████████▉| 101/102 [00:15<00:00, 22.00it/s, est. speed input: 10798.41 toks/s, output: 1036.51 toks/s]
Processed prompts: 100%|██████████| 102/102 [00:18<00:00,  5.55it/s, est. speed input: 9424.68 toks/s, output: 913.02 toks/s]  
[rank0]:[W711 15:22:34.834946619 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/asym/ae_rag_lp_sc_test_75_6_output.json
Cleaned temporary files.
Keeping ['ae', 'sc'] in profile
Temp data file saved at data/processed/TEMP_2025-07-11 15:22:37.889338_lp_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-11 15:22:37.889338_lp_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-11 15:22:37.889338_lp_processed_test.json', '--batch_size', '4']

  0%|          | 0/136 [00:00<?, ?it/s]
  1%|          | 1/136 [00:00<00:41,  3.23it/s]
  2%|▏         | 3/136 [00:00<00:20,  6.36it/s]
  3%|▎         | 4/136 [00:01<01:08,  1.93it/s]
  4%|▎         | 5/136 [00:02<01:14,  1.75it/s]
  4%|▍         | 6/136 [00:02<00:56,  2.29it/s]
  6%|▌         | 8/136 [00:02<00:34,  3.66it/s]
  7%|▋         | 10/136 [00:03<00:27,  4.50it/s]
  8%|▊         | 11/136 [00:03<00:29,  4.22it/s]
  9%|▉         | 12/136 [00:03<00:27,  4.50it/s]
 10%|▉         | 13/136 [00:03<00:23,  5.21it/s]
 11%|█         | 15/136 [00:04<00:27,  4.38it/s]
 12%|█▏        | 16/136 [00:04<00:39,  3.00it/s]
 12%|█▎        | 17/136 [00:05<00:49,  2.40it/s]
 13%|█▎        | 18/136 [00:05<00:42,  2.79it/s]
 14%|█▍        | 19/136 [00:06<00:43,  2.67it/s]
 15%|█▍        | 20/136 [00:06<00:48,  2.38it/s]
 16%|█▌        | 22/136 [00:06<00:30,  3.68it/s]
 18%|█▊        | 25/136 [00:08<00:42,  2.60it/s]
 19%|█▉        | 26/136 [00:08<00:38,  2.86it/s]
 21%|██        | 28/136 [00:08<00:27,  3.96it/s]
 21%|██▏       | 29/136 [00:09<00:29,  3.62it/s]
 22%|██▏       | 30/136 [00:09<00:27,  3.83it/s]
 23%|██▎       | 31/136 [00:09<00:25,  4.06it/s]
 24%|██▎       | 32/136 [00:09<00:31,  3.33it/s]
 24%|██▍       | 33/136 [00:10<00:25,  4.00it/s]
 26%|██▌       | 35/136 [00:10<00:18,  5.48it/s]
 26%|██▋       | 36/136 [00:10<00:16,  6.13it/s]
 28%|██▊       | 38/136 [00:10<00:14,  6.85it/s]
 30%|███       | 41/136 [00:10<00:11,  8.14it/s]
 32%|███▏      | 44/136 [00:11<00:09,  9.63it/s]
 34%|███▍      | 46/136 [00:12<00:25,  3.57it/s]
 35%|███▌      | 48/136 [00:13<00:25,  3.45it/s]
 36%|███▌      | 49/136 [00:13<00:29,  2.96it/s]
 38%|███▊      | 52/136 [00:13<00:17,  4.75it/s]
 40%|███▉      | 54/136 [00:14<00:19,  4.27it/s]
 41%|████      | 56/136 [00:14<00:15,  5.24it/s]
 43%|████▎     | 58/136 [00:14<00:13,  5.71it/s]
 43%|████▎     | 59/136 [00:14<00:12,  6.17it/s]
 44%|████▍     | 60/136 [00:15<00:13,  5.60it/s]
 45%|████▍     | 61/136 [00:15<00:12,  5.97it/s]
 46%|████▋     | 63/136 [00:15<00:09,  8.07it/s]
 48%|████▊     | 65/136 [00:15<00:12,  5.91it/s]
 49%|████▊     | 66/136 [00:16<00:12,  5.72it/s]
 50%|█████     | 68/136 [00:16<00:09,  7.13it/s]
 51%|█████     | 69/136 [00:17<00:17,  3.81it/s]
 51%|█████▏    | 70/136 [00:17<00:20,  3.24it/s]
 52%|█████▏    | 71/136 [00:17<00:22,  2.90it/s]
 53%|█████▎    | 72/136 [00:18<00:23,  2.72it/s]
 54%|█████▎    | 73/136 [00:18<00:22,  2.85it/s]
 55%|█████▌    | 75/136 [00:19<00:18,  3.29it/s]
 57%|█████▋    | 77/136 [00:20<00:25,  2.30it/s]
 57%|█████▋    | 78/136 [00:21<00:35,  1.65it/s]
 58%|█████▊    | 79/136 [00:22<00:42,  1.34it/s]
 59%|█████▉    | 80/136 [00:24<00:48,  1.15it/s]
 60%|█████▉    | 81/136 [00:25<00:53,  1.04it/s]
 60%|██████    | 82/136 [00:26<00:56,  1.04s/it]
 61%|██████    | 83/136 [00:26<00:41,  1.27it/s]
 62%|██████▏   | 84/136 [00:26<00:31,  1.67it/s]
 62%|██████▎   | 85/136 [00:28<00:40,  1.27it/s]
 63%|██████▎   | 86/136 [00:29<00:45,  1.09it/s]
 64%|██████▍   | 87/136 [00:29<00:38,  1.27it/s]
 65%|██████▍   | 88/136 [00:30<00:32,  1.47it/s]
 66%|██████▌   | 90/136 [00:30<00:20,  2.29it/s]
 68%|██████▊   | 92/136 [00:30<00:12,  3.47it/s]
 68%|██████▊   | 93/136 [00:31<00:12,  3.50it/s]
 69%|██████▉   | 94/136 [00:31<00:14,  2.97it/s]
 70%|██████▉   | 95/136 [00:32<00:17,  2.35it/s]
 71%|███████   | 96/136 [00:32<00:17,  2.32it/s]
 72%|███████▏  | 98/136 [00:32<00:10,  3.54it/s]
 74%|███████▎  | 100/136 [00:33<00:07,  4.61it/s]
 75%|███████▌  | 102/136 [00:33<00:07,  4.31it/s]
 76%|███████▋  | 104/136 [00:33<00:05,  5.37it/s]
 78%|███████▊  | 106/136 [00:33<00:04,  6.78it/s]
 79%|███████▉  | 108/136 [00:35<00:09,  2.98it/s]
 80%|████████  | 109/136 [00:37<00:18,  1.46it/s]
 82%|████████▏ | 111/136 [00:38<00:13,  1.84it/s]
 82%|████████▏ | 112/136 [00:38<00:11,  2.11it/s]
 83%|████████▎ | 113/136 [00:39<00:12,  1.89it/s]
 84%|████████▍ | 114/136 [00:39<00:10,  2.16it/s]
 85%|████████▍ | 115/136 [00:39<00:08,  2.41it/s]
 86%|████████▌ | 117/136 [00:39<00:05,  3.78it/s]
 87%|████████▋ | 118/136 [00:39<00:04,  3.70it/s]
 88%|████████▊ | 119/136 [00:40<00:04,  4.18it/s]
 88%|████████▊ | 120/136 [00:40<00:03,  4.02it/s]
 90%|████████▉ | 122/136 [00:40<00:02,  4.80it/s]
 91%|█████████ | 124/136 [00:41<00:03,  3.69it/s]
 93%|█████████▎| 126/136 [00:42<00:02,  3.56it/s]
 93%|█████████▎| 127/136 [00:43<00:04,  2.11it/s]
 95%|█████████▍| 129/136 [00:43<00:02,  3.10it/s]
 96%|█████████▋| 131/136 [00:45<00:03,  1.63it/s]
 98%|█████████▊| 133/136 [00:45<00:01,  2.24it/s]
 99%|█████████▉| 135/136 [00:46<00:00,  2.58it/s]
100%|██████████| 136/136 [00:47<00:00,  2.25it/s]
100%|██████████| 136/136 [00:47<00:00,  2.88it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-11 15:22:37.889338_lp_processed_test.json', '--output_addr', 'data/out/rag/asym/lp_rag_ae_sc_test_75_6_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '6', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 136 examples [00:00, 1303.31 examples/s]
INFO 07-11 15:23:52 config.py:510] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 07-11 15:23:52 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-11 15:23:54 selector.py:120] Using Flash Attention backend.
INFO 07-11 15:23:55 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-11 15:23:55 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.44it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.37it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.41it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]

INFO 07-11 15:23:58 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-11 15:24:00 worker.py:241] Memory profiling takes 2.22 seconds
INFO 07-11 15:24:00 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-11 15:24:00 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-11 15:24:01 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-11 15:24:01 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-11 15:24:04 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.08it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:15,  2.16it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:15,  2.12it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.15it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.15it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.15it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.17it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.15it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:12,  2.16it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.18it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:11,  2.17it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.19it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:10,  2.16it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.17it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.19it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.13it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.17it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.20it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.19it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.22it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.20it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:05,  2.21it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.24it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:11<00:04,  2.21it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.18it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:04,  2.21it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.21it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.23it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.21it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.23it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.24it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.22it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:15<00:00,  2.21it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.24it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.22it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.19it/s]
INFO 07-11 15:24:20 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.22 GiB
INFO 07-11 15:24:20 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.12 seconds
None

Processed prompts:   0%|          | 0/136 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/136 [00:14<31:50, 14.15s/it, est. speed input: 65.66 toks/s, output: 7.42 toks/s]
Processed prompts:   1%|▏         | 2/136 [00:14<13:16,  5.94s/it, est. speed input: 235.61 toks/s, output: 14.99 toks/s]
Processed prompts:   2%|▏         | 3/136 [00:14<07:48,  3.53s/it, est. speed input: 316.13 toks/s, output: 22.14 toks/s]
Processed prompts:   3%|▎         | 4/136 [00:15<04:47,  2.18s/it, est. speed input: 369.05 toks/s, output: 29.90 toks/s]
Processed prompts:   5%|▌         | 7/136 [00:15<01:47,  1.20it/s, est. speed input: 596.74 toks/s, output: 53.71 toks/s]
Processed prompts:   7%|▋         | 9/136 [00:15<01:09,  1.82it/s, est. speed input: 715.16 toks/s, output: 69.51 toks/s]
Processed prompts:   9%|▉         | 12/136 [00:15<00:39,  3.12it/s, est. speed input: 975.08 toks/s, output: 94.09 toks/s]
Processed prompts:  14%|█▍        | 19/136 [00:15<00:16,  7.26it/s, est. speed input: 1762.55 toks/s, output: 152.31 toks/s]
Processed prompts:  19%|█▉        | 26/136 [00:15<00:09, 12.04it/s, est. speed input: 2240.96 toks/s, output: 211.21 toks/s]
Processed prompts:  23%|██▎       | 31/136 [00:15<00:06, 15.47it/s, est. speed input: 2681.27 toks/s, output: 253.43 toks/s]
Processed prompts:  26%|██▋       | 36/136 [00:15<00:05, 19.14it/s, est. speed input: 3052.48 toks/s, output: 296.65 toks/s]
Processed prompts:  32%|███▏      | 44/136 [00:16<00:03, 27.23it/s, est. speed input: 3690.38 toks/s, output: 367.38 toks/s]
Processed prompts:  38%|███▊      | 52/136 [00:16<00:02, 35.16it/s, est. speed input: 4234.29 toks/s, output: 439.57 toks/s]
Processed prompts:  46%|████▌     | 62/136 [00:16<00:01, 46.53it/s, est. speed input: 5065.43 toks/s, output: 531.63 toks/s]
Processed prompts:  51%|█████     | 69/136 [00:16<00:01, 50.77it/s, est. speed input: 5677.97 toks/s, output: 596.12 toks/s]
Processed prompts:  56%|█████▌    | 76/136 [00:16<00:01, 54.73it/s, est. speed input: 6264.70 toks/s, output: 661.66 toks/s]
Processed prompts:  63%|██████▎   | 86/136 [00:16<00:00, 62.82it/s, est. speed input: 7013.22 toks/s, output: 757.52 toks/s]
Processed prompts:  69%|██████▉   | 94/136 [00:16<00:00, 59.33it/s, est. speed input: 7540.79 toks/s, output: 833.20 toks/s]
Processed prompts:  74%|███████▍  | 101/136 [00:16<00:00, 51.76it/s, est. speed input: 8002.04 toks/s, output: 899.19 toks/s]
Processed prompts:  81%|████████  | 110/136 [00:17<00:00, 57.00it/s, est. speed input: 8709.18 toks/s, output: 992.00 toks/s]
Processed prompts:  86%|████████▌ | 117/136 [00:17<00:00, 50.30it/s, est. speed input: 9223.30 toks/s, output: 1062.55 toks/s]
Processed prompts:  90%|█████████ | 123/136 [00:17<00:00, 42.01it/s, est. speed input: 9693.04 toks/s, output: 1121.93 toks/s]
Processed prompts:  94%|█████████▍| 128/136 [00:17<00:00, 33.84it/s, est. speed input: 9880.13 toks/s, output: 1170.39 toks/s]
Processed prompts:  97%|█████████▋| 132/136 [00:18<00:00, 17.76it/s, est. speed input: 9840.38 toks/s, output: 1186.92 toks/s]
Processed prompts:  99%|█████████▉| 135/136 [00:20<00:00,  6.66it/s, est. speed input: 9220.63 toks/s, output: 1139.21 toks/s]
Processed prompts: 100%|██████████| 136/136 [00:20<00:00,  6.75it/s, est. speed input: 9245.92 toks/s, output: 1157.52 toks/s]
[rank0]:[W711 15:24:43.797177503 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/asym/lp_rag_ae_sc_test_75_6_output.json
Cleaned temporary files.
Keeping ['ae', 'lp'] in profile
Temp data file saved at data/processed/TEMP_2025-07-11 15:24:46.838145_sc_processed_test.json.
	Running Ranking: ['python', 'retrieval/rank_dataset.py', '--model_name', 'facebook/contriever-msmarco', '--input_dataset_addr', 'data/processed/TEMP_2025-07-11 15:24:46.838145_sc_processed_test.json', '--output_dataset_addr', 'data/processed/TEMP_2025-07-11 15:24:46.838145_sc_processed_test.json', '--batch_size', '4']

  0%|          | 0/106 [00:00<?, ?it/s]
  1%|          | 1/106 [00:00<00:37,  2.78it/s]
  3%|▎         | 3/106 [00:00<00:17,  5.96it/s]
  4%|▍         | 4/106 [00:00<00:22,  4.52it/s]
  5%|▍         | 5/106 [00:01<00:20,  4.87it/s]
  7%|▋         | 7/106 [00:01<00:16,  5.97it/s]
  8%|▊         | 8/106 [00:02<00:29,  3.31it/s]
  8%|▊         | 9/106 [00:02<00:24,  4.01it/s]
 10%|█         | 11/106 [00:02<00:16,  5.75it/s]
 14%|█▍        | 15/106 [00:02<00:09,  9.88it/s]
 16%|█▌        | 17/106 [00:02<00:11,  8.07it/s]
 18%|█▊        | 19/106 [00:02<00:08,  9.67it/s]
 20%|█▉        | 21/106 [00:03<00:13,  6.14it/s]
 22%|██▏       | 23/106 [00:04<00:17,  4.81it/s]
 23%|██▎       | 24/106 [00:04<00:22,  3.70it/s]
 24%|██▎       | 25/106 [00:04<00:20,  3.91it/s]
 25%|██▌       | 27/106 [00:05<00:20,  3.85it/s]
 27%|██▋       | 29/106 [00:05<00:15,  5.01it/s]
 29%|██▉       | 31/106 [00:05<00:11,  6.28it/s]
 31%|███       | 33/106 [00:05<00:09,  7.54it/s]
 33%|███▎      | 35/106 [00:06<00:09,  7.15it/s]
 34%|███▍      | 36/106 [00:07<00:19,  3.63it/s]
 35%|███▍      | 37/106 [00:09<00:47,  1.46it/s]
 37%|███▋      | 39/106 [00:09<00:30,  2.18it/s]
 38%|███▊      | 40/106 [00:09<00:26,  2.53it/s]
 39%|███▊      | 41/106 [00:10<00:27,  2.37it/s]
 40%|███▉      | 42/106 [00:10<00:22,  2.83it/s]
 41%|████      | 43/106 [00:10<00:18,  3.40it/s]
 42%|████▏     | 45/106 [00:10<00:12,  4.96it/s]
 43%|████▎     | 46/106 [00:11<00:16,  3.62it/s]
 44%|████▍     | 47/106 [00:11<00:14,  3.97it/s]
 45%|████▌     | 48/106 [00:11<00:14,  4.11it/s]
 46%|████▌     | 49/106 [00:12<00:19,  2.91it/s]
 47%|████▋     | 50/106 [00:12<00:16,  3.38it/s]
 49%|████▉     | 52/106 [00:12<00:10,  5.10it/s]
 50%|█████     | 53/106 [00:12<00:14,  3.62it/s]
 51%|█████     | 54/106 [00:13<00:14,  3.70it/s]
 52%|█████▏    | 55/106 [00:13<00:17,  2.90it/s]
 53%|█████▎    | 56/106 [00:14<00:15,  3.15it/s]
 54%|█████▍    | 57/106 [00:14<00:12,  3.89it/s]
 55%|█████▍    | 58/106 [00:14<00:10,  4.41it/s]
 56%|█████▌    | 59/106 [00:14<00:14,  3.17it/s]
 58%|█████▊    | 61/106 [00:15<00:10,  4.10it/s]
 59%|█████▉    | 63/106 [00:15<00:07,  5.57it/s]
 62%|██████▏   | 66/106 [00:15<00:05,  7.13it/s]
 64%|██████▍   | 68/106 [00:15<00:04,  8.18it/s]
 65%|██████▌   | 69/106 [00:15<00:04,  8.43it/s]
 66%|██████▌   | 70/106 [00:16<00:09,  3.82it/s]
 68%|██████▊   | 72/106 [00:16<00:06,  4.89it/s]
 69%|██████▉   | 73/106 [00:17<00:06,  5.08it/s]
 70%|██████▉   | 74/106 [00:17<00:09,  3.22it/s]
 71%|███████   | 75/106 [00:17<00:08,  3.49it/s]
 72%|███████▏  | 76/106 [00:18<00:07,  3.79it/s]
 73%|███████▎  | 77/106 [00:18<00:07,  3.84it/s]
 75%|███████▍  | 79/106 [00:18<00:05,  4.61it/s]
 75%|███████▌  | 80/106 [00:19<00:09,  2.77it/s]
 77%|███████▋  | 82/106 [00:20<00:09,  2.61it/s]
 78%|███████▊  | 83/106 [00:21<00:10,  2.16it/s]
 79%|███████▉  | 84/106 [00:21<00:08,  2.57it/s]
 80%|████████  | 85/106 [00:21<00:07,  2.76it/s]
 82%|████████▏ | 87/106 [00:21<00:04,  4.35it/s]
 83%|████████▎ | 88/106 [00:21<00:04,  4.27it/s]
 85%|████████▍ | 90/106 [00:22<00:03,  4.07it/s]
 86%|████████▌ | 91/106 [00:22<00:03,  3.91it/s]
 88%|████████▊ | 93/106 [00:22<00:02,  5.20it/s]
 89%|████████▊ | 94/106 [00:23<00:02,  5.55it/s]
 92%|█████████▏| 97/106 [00:23<00:01,  6.79it/s]
 92%|█████████▏| 98/106 [00:23<00:01,  5.73it/s]
 93%|█████████▎| 99/106 [00:24<00:01,  3.69it/s]
 94%|█████████▍| 100/106 [00:24<00:02,  2.79it/s]
 95%|█████████▌| 101/106 [00:25<00:01,  3.06it/s]
 96%|█████████▌| 102/106 [00:25<00:01,  3.55it/s]
 97%|█████████▋| 103/106 [00:25<00:01,  2.61it/s]
 99%|█████████▉| 105/106 [00:26<00:00,  2.56it/s]
100%|██████████| 106/106 [00:27<00:00,  2.34it/s]
100%|██████████| 106/106 [00:27<00:00,  3.88it/s]
	Running baseline: ['python', 'baselines.py', '--model_addr', 'Qwen/Qwen2.5-7B-Instruct', '--inputs_addr', 'data/processed/TEMP_2025-07-11 15:24:46.838145_sc_processed_test.json', '--output_addr', 'data/out/rag/asym/sc_rag_ae_lp_test_75_6_output.json', '--temperature', '0.1', '--top_p', '0.95', '--max_tokens', '8192', '--num_generated_outputs', '1', '--num_contexts', '6', '--max_retries', '10', '--cache_dir', '/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', '--rag']

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 106 examples [00:00, 1318.64 examples/s]
INFO 07-11 15:25:39 config.py:510] This model supports multiple tasks: {'reward', 'embed', 'score', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 07-11 15:25:39 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scratch3/workspace/oyilmazel_umass_edu-lampqa_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-11 15:25:40 selector.py:120] Using Flash Attention backend.
INFO 07-11 15:25:42 model_runner.py:1094] Starting to load model Qwen/Qwen2.5-7B-Instruct...
INFO 07-11 15:25:42 weight_utils.py:251] Using model weights format ['*.safetensors']

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.43it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.36it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.41it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]

INFO 07-11 15:25:45 model_runner.py:1099] Loading model weights took 14.2487 GB
INFO 07-11 15:25:47 worker.py:241] Memory profiling takes 2.20 seconds
INFO 07-11 15:25:47 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 07-11 15:25:47 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.
INFO 07-11 15:25:47 gpu_executor.py:76] # GPU blocks: 61542, # CPU blocks: 4681
INFO 07-11 15:25:47 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x
INFO 07-11 15:25:51 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:17,  1.99it/s]
Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:16,  2.05it/s]
Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:15,  2.11it/s]
Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.16it/s]
Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:14,  2.12it/s]
Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.16it/s]
Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.17it/s]
Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.16it/s]
Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:11,  2.18it/s]
Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.17it/s]
Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:10,  2.18it/s]
Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.20it/s]
Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:10,  2.16it/s]
Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.18it/s]
Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.18it/s]
Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.17it/s]
Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.20it/s]
Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.20it/s]
Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.22it/s]
Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.24it/s]
Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.20it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:05,  2.23it/s]
Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.25it/s]
Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.23it/s]
Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.22it/s]
Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:04,  2.20it/s]
Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.23it/s]
Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.25it/s]
Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.22it/s]
Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.25it/s]
Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.27it/s]
Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.24it/s]
Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:15<00:00,  2.23it/s]
Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.18it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.19it/s]
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.19it/s]
INFO 07-11 15:26:07 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.22 GiB
INFO 07-11 15:26:07 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.06 seconds
None

Processed prompts:   0%|          | 0/106 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/106 [00:10<18:52, 10.78s/it, est. speed input: 118.14 toks/s, output: 6.49 toks/s]
Processed prompts:   2%|▏         | 2/106 [00:11<08:24,  4.86s/it, est. speed input: 225.42 toks/s, output: 14.10 toks/s]
Processed prompts:   3%|▎         | 3/106 [00:11<04:37,  2.70s/it, est. speed input: 451.70 toks/s, output: 22.20 toks/s]
Processed prompts:   5%|▍         | 5/106 [00:11<02:06,  1.25s/it, est. speed input: 754.79 toks/s, output: 38.72 toks/s]
Processed prompts:   7%|▋         | 7/106 [00:12<01:11,  1.38it/s, est. speed input: 1294.07 toks/s, output: 56.21 toks/s]
Processed prompts:   8%|▊         | 8/106 [00:12<00:56,  1.72it/s, est. speed input: 1379.55 toks/s, output: 64.99 toks/s]
Processed prompts:   9%|▉         | 10/106 [00:12<00:35,  2.67it/s, est. speed input: 1505.76 toks/s, output: 83.15 toks/s]
Processed prompts:  11%|█▏        | 12/106 [00:12<00:24,  3.86it/s, est. speed input: 1711.42 toks/s, output: 102.00 toks/s]
Processed prompts:  13%|█▎        | 14/106 [00:12<00:18,  4.86it/s, est. speed input: 1964.22 toks/s, output: 120.68 toks/s]
Processed prompts:  15%|█▌        | 16/106 [00:12<00:14,  6.34it/s, est. speed input: 2108.09 toks/s, output: 140.34 toks/s]
Processed prompts:  23%|██▎       | 24/106 [00:12<00:05, 15.00it/s, est. speed input: 2742.16 toks/s, output: 222.52 toks/s]
Processed prompts:  26%|██▋       | 28/106 [00:13<00:04, 18.38it/s, est. speed input: 3251.07 toks/s, output: 263.74 toks/s]
Processed prompts:  31%|███       | 33/106 [00:13<00:03, 22.40it/s, est. speed input: 3755.64 toks/s, output: 316.20 toks/s]
Processed prompts:  39%|███▊      | 41/106 [00:13<00:01, 32.88it/s, est. speed input: 4627.48 toks/s, output: 404.10 toks/s]
Processed prompts:  43%|████▎     | 46/106 [00:13<00:01, 34.64it/s, est. speed input: 5123.47 toks/s, output: 457.86 toks/s]
Processed prompts:  51%|█████     | 54/106 [00:13<00:01, 40.87it/s, est. speed input: 5812.86 toks/s, output: 547.15 toks/s]
Processed prompts:  57%|█████▋    | 60/106 [00:13<00:01, 43.75it/s, est. speed input: 6282.03 toks/s, output: 615.48 toks/s]
Processed prompts:  64%|██████▍   | 68/106 [00:13<00:00, 51.54it/s, est. speed input: 6998.40 toks/s, output: 709.54 toks/s]
Processed prompts:  77%|███████▋  | 82/106 [00:13<00:00, 71.53it/s, est. speed input: 8607.19 toks/s, output: 881.09 toks/s]
Processed prompts:  85%|████████▍ | 90/106 [00:14<00:00, 60.44it/s, est. speed input: 9440.44 toks/s, output: 974.83 toks/s]
Processed prompts:  92%|█████████▏| 97/106 [00:14<00:00, 45.22it/s, est. speed input: 9889.72 toks/s, output: 1053.97 toks/s]
Processed prompts:  97%|█████████▋| 103/106 [00:16<00:00, 11.12it/s, est. speed input: 9324.33 toks/s, output: 1027.03 toks/s]
Processed prompts: 100%|██████████| 106/106 [00:16<00:00,  6.39it/s, est. speed input: 9516.31 toks/s, output: 1063.03 toks/s]
[rank0]:[W711 15:26:26.771260997 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Experiment complete! Output can be found at data/out/rag/asym/sc_rag_ae_lp_test_75_6_output.json
Cleaned temporary files.
